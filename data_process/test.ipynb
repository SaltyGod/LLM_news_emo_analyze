{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0430 Qwen部分修改**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/root/LLM_news_emo_analyze/DATA/deepseek_sft_0419data/ds_train_data.csv',encoding = 'utf_8_sig',lineterminator='\\n')\n",
    "test = pd.read_csv('/root/LLM_news_emo_analyze/DATA/deepseek_sft_0419data/ds_valid_data.csv',encoding = 'utf_8_sig',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"**角色定义**\n",
    "你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的新闻文本情绪进行准确的评分：\n",
    "\n",
    "**任务流程**\n",
    "1. 分析新闻整体内容情绪\n",
    "2. 匹配情绪词典关键词，对情绪进行深入理解与分析\n",
    "3. 输出情绪分析跟对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0，分别代表非常消极、比较消极、中性、比较积极、非常积极）\n",
    "\n",
    "**评分逻辑**\n",
    "1. 语义匹配：忽略偏离的情绪词词汇，保留有效情绪词\n",
    "2. 评分调整：整体语义优先，情绪词辅助修正\n",
    "\n",
    "**示例说明**\n",
    "- 示例1：\n",
    "新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\n",
    "情绪词典：profitability:0.6,profit:0.8\n",
    "情绪分析:①语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破；②关键情绪词调整：“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1.0分（非常积极）的评分结果。\n",
    "情绪得分:1.0\n",
    "\n",
    "- 示例2：\n",
    "新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\n",
    "情绪词典：mispricing:-0.4,advantage:0.7\n",
    "情绪分析:①语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号；②关键情绪词调整：未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。由于缺乏词典强匹配项限制进一步上调空间，整体乐观基调符合“比较积极”，情绪得分0.5。\n",
    "情绪得分:0.5\n",
    "\n",
    "- 示例3：\n",
    "新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\n",
    "情绪词典：free:0.2,immateriality:-0.2\n",
    "情绪分析:①语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响；②关键情绪词调整：“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0.0分。\n",
    "情绪得分:0.0\n",
    "\n",
    "- 示例4：\n",
    "新闻文本：我们投入的前期费用谁来承担\n",
    "情绪词典：invest:0.3\n",
    "情绪分析:①语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪；②关键情绪词调整：“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位“比较消极”，即情绪得分-0.5分。\n",
    "情绪得分:-0.5\n",
    "\n",
    "- 示例5：\n",
    "新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\n",
    "情绪词典：crash:-0.9,meltdown:-0.8\n",
    "情绪分析:①语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词调整：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，因此是非常消极，情绪得分-1.0分。\n",
    "情绪得分:-1.0\n",
    "\n",
    "\n",
    "**其他说明**\n",
    "- 情绪词典的分值仅作语义方向参考\n",
    "- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分数\n",
    "- 输出格式：{\"情绪分析\":\"...\",\"情绪得分\":\"...\"}\n",
    "\n",
    "现在，请你开始分析并按照要求输出结果：\n",
    "新闻文本：{{新闻文本}}\n",
    "情绪词典：{{情绪词}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['qwen_input'] = train.apply(\n",
    "    lambda row: prompt.replace('{{新闻文本}}', row['cut_news'] + ('。' if not row['cut_news'].endswith('。') else ''))\n",
    "              .replace('{{情绪词}}', row['sentiment_dict_v2']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test['qwen_input'] = test.apply(\n",
    "    lambda row: prompt.replace('{{新闻文本}}', row['cut_news'] + ('。' if not row['cut_news'].endswith('。') else ''))\n",
    "              .replace('{{情绪词}}', row['sentiment_dict_v2']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_number(text):\n",
    "#     pattern = r':(-?\\d+(?:\\.\\d+)?)'\n",
    "#     match = re.search(pattern, text)\n",
    "#     if match:\n",
    "#         return float(match.group(1))\n",
    "#     return None\n",
    "\n",
    "# # 应用函数到 content 列并创建新列 score\n",
    "# data['score'] = data['content'].apply(extract_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_sentiment_score(text):\n",
    "    # 匹配的情绪得分值（注意：用 r'\\b' 保证是独立的数字）\n",
    "    pattern = r'(?<!\\d)(-1\\.0|-0\\.5|0\\.5|1\\.0|-1|0\\.0|1|0)(?!\\d)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        return matches[-1]  # 返回最后一个匹配值\n",
    "    return None  # 没有匹配则返回 None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['final_score'] = train['Complex_CoT'].apply(extract_last_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不一致率为：4.78%（529/11065）\n",
      "不一致的样例如下：\n",
      "    final_score  score\n",
      "37          0.5    0.0\n",
      "171         0.5    0.0\n",
      "177           1    0.5\n",
      "190           1    0.5\n",
      "209           1    0.5\n",
      "218           1    0.5\n",
      "219           1    0.5\n",
      "299         0.5    0.0\n",
      "317           0   -0.5\n",
      "319         0.5    0.0\n",
      "334         0.5    0.0\n",
      "357           1    0.5\n",
      "386         0.5    0.0\n",
      "391         0.5    0.0\n",
      "423           1    0.5\n",
      "424         0.5    0.0\n",
      "432           1    0.5\n",
      "457         0.5    0.0\n",
      "460         0.5    0.0\n",
      "522           1    0.5\n",
      "533         0.5    0.0\n",
      "569           1    0.5\n",
      "599         0.5    0.0\n",
      "633         0.5    0.0\n",
      "637         0.5    0.0\n",
      "647           1    0.5\n",
      "693           1    0.5\n",
      "711           1    0.5\n",
      "744         0.5    0.0\n",
      "757           1    0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 步骤1：统一格式（去除空格，转换为字符串再转为 float）\n",
    "def normalize_score(val):\n",
    "    try:\n",
    "        return float(str(val).strip())\n",
    "    except:\n",
    "        return None  # 或者 np.nan，如果你想标记无效值\n",
    "\n",
    "train['final_score_clean'] = train['final_score'].apply(normalize_score)\n",
    "train['score_clean'] = train['score'].apply(normalize_score)\n",
    "\n",
    "# 步骤2：计算不一致行（用 np.isclose 更稳健，考虑浮点误差）\n",
    "import numpy as np\n",
    "\n",
    "train['is_equal'] = np.isclose(train['final_score_clean'], train['score_clean'], atol=1e-6)\n",
    "\n",
    "# 步骤3：计算不一致率\n",
    "total = len(train)\n",
    "inconsistent = (~train['is_equal']).sum()\n",
    "inconsistency_rate = inconsistent / total\n",
    "\n",
    "print(f'不一致率为：{inconsistency_rate:.2%}（{inconsistent}/{total}）')\n",
    "\n",
    "# 步骤4：输出不一致的行\n",
    "diff_rows = train[~train['is_equal']]\n",
    "print(\"不一致的样例如下：\")\n",
    "print(diff_rows[['final_score', 'score']].head(30))  # 可根据需要显示更多行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['final_score', 'score_clean', 'is_equal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>NewsSource</th>\n",
       "      <th>stratum</th>\n",
       "      <th>topic</th>\n",
       "      <th>similarity</th>\n",
       "      <th>cut_news</th>\n",
       "      <th>sentiment_dict</th>\n",
       "      <th>sentiment_dict_v2</th>\n",
       "      <th>Response</th>\n",
       "      <th>Complex_CoT</th>\n",
       "      <th>score</th>\n",
       "      <th>Question</th>\n",
       "      <th>qwen_input</th>\n",
       "      <th>final_score_clean</th>\n",
       "      <th>qwen_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>下半场还是熊来了 牛市底盘裂缝</td>\n",
       "      <td>经济观察报</td>\n",
       "      <td>2008-03-17 08:23:00_经济观察报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.480989</td>\n",
       "      <td>易方达的一位基金经理表示，机构投资者在较早时期便开始减仓，目前市场更多表现为中小投资者的恐慌...</td>\n",
       "      <td>撤资/抛售(divestment):-0.2,清仓/抛售(closeouts):-0.2</td>\n",
       "      <td>divestment:-0.2, closeouts:-0.2</td>\n",
       "      <td>{\"情绪得分\": \"-1\"}</td>\n",
       "      <td>好，我需要分析这篇新闻的情绪。新闻中提到易方达的基金经理说机构投资者早前就开始减仓，现在市场...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>{\"情绪分析\":\"好，我需要分析这篇新闻的情绪。新闻中提到易方达的基金经理说机构投资者早前就...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>风投的另类生意</td>\n",
       "      <td>经济日报</td>\n",
       "      <td>2014-05-02 00:00:00_经济日报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.491875</td>\n",
       "      <td>“风投采取PIPE投资能增加PE资本流动性，提高PE资本的变现能力</td>\n",
       "      <td>资本/资金(capital):0.1,投资/投资额(investment):0.4</td>\n",
       "      <td>capital:0.1, investment:0.4</td>\n",
       "      <td>{\"情绪得分\": \"0.5\"}</td>\n",
       "      <td>好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“风投采取PIPE投资能增加PE资本流...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“风投采取PIPE投...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>投资者结构分化中的贵州茅台：千元股价贵不贵？</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>2019-06-24 00:00:00_第一财经日报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.524268</td>\n",
       "      <td>”深圳一位公募基金经理也表示</td>\n",
       "      <td>基金/资金(fund):0.1,基础/底部(base):0.0</td>\n",
       "      <td>fund:0.1, base:0.0</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我现在需要分析给定的新闻文本的情绪得分。首先，新闻文本是：“深圳一位公募基金经理也表示...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要分析给定的新闻文本的情绪得分。首先，新闻文本是：“深圳一位...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32次降价失败后 购销“两票制”能让药价瘦身吗</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>2017-01-10 00:00:00_第一财经日报</td>\n",
       "      <td>“生产投资”</td>\n",
       "      <td>0.474199</td>\n",
       "      <td>这结果值得我们反思，根源是否就在于招采的制度和机制</td>\n",
       "      <td>选矿/精选(beneficiation):0.4,管理不善/管理失误(mismanageme...</td>\n",
       "      <td>beneficiation:0.4, mismanagement:-0.5</td>\n",
       "      <td>{\"情绪得分\": \"-0.5\"}</td>\n",
       "      <td>好的，我需要分析这条新闻的情绪得分。首先看一下新闻内容：“这结果值得我们反思，根源是否就在于...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>{\"情绪分析\":\"好的，我需要分析这条新闻的情绪得分。首先看一下新闻内容：“这结果值得我们反...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>用实验探索贫穷的本质</td>\n",
       "      <td>经济观察报</td>\n",
       "      <td>2019-10-19 02:09:00_经济观察报</td>\n",
       "      <td>“宏观经济”</td>\n",
       "      <td>0.505570</td>\n",
       "      <td>这种人民币形态的热钱规模有多大</td>\n",
       "      <td>美分/分币(cent):0.0,货币/通货(currency):0.0</td>\n",
       "      <td>cent:0.0, currency:0.0</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“这种人民币形态的热钱规模有多大。”这...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“这种人民币形态的热...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11060</th>\n",
       "      <td>退市新规精准发力：有效出清绩差风险公司 ST股数量总体稳定</td>\n",
       "      <td>21世纪经济报道</td>\n",
       "      <td>2024-06-06 00:00:00_21世纪经济报道</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.594900</td>\n",
       "      <td>同时，坚持对包括退市公司在内的财务造假公司及相关责任人违法违规行为“一追到底”、依法严惩，今...</td>\n",
       "      <td>应受惩罚的/可惩处的(punishable):-0.4,公司的/企业的(corporate)...</td>\n",
       "      <td>punishable:-0.4, corporate:0.0</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我现在需要分析这段中文新闻的情绪，并给出五档制评分。首先，先仔细阅读新闻内容。\\n\\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要分析这段中文新闻的情绪，并给出五档制评分。首先，先仔细阅读...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11061</th>\n",
       "      <td>提升电气化水平 驱动经济社会高质量发展</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>2020-06-15 00:00:00_经济参考报</td>\n",
       "      <td>“生产投资”</td>\n",
       "      <td>0.488712</td>\n",
       "      <td>在城乡居民用电领域，加大北方地区清洁取暖工作力度，提升清洁能源供热能力，发挥电供暖示范作用</td>\n",
       "      <td>北方/北部(north):0.0,电的/电气的(electrical):0.0</td>\n",
       "      <td>north:0.0, electrical:0.0</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我需要分析这段关于中国城乡居民用电领域的新闻文本的情绪，并根据提供的评分标准给出五档制...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"好的，我需要分析这段关于中国城乡居民用电领域的新闻文本的情绪，并根据提供的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11062</th>\n",
       "      <td>“浑水”如何搅起一池浑水</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>2011-11-25 00:00:00_第一财经日报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>业内人士透露，集体诉讼原告费用一般由律师预支，采取风险代理模式，如果诉讼成功，律师事务所能够...</td>\n",
       "      <td>诉讼当事人(litigant):0.0,诉讼/官司(lawsuit):-0.2</td>\n",
       "      <td>litigant:0.0, lawsuit:-0.2</td>\n",
       "      <td>{\"情绪得分\": \"-0.5\"}</td>\n",
       "      <td>好，我需要分析这篇新闻的情绪，然后根据给定的情绪词典和评分标准来打分。首先，仔细阅读新闻内容...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"好，我需要分析这篇新闻的情绪，然后根据给定的情绪词典和评分标准来打分。首先...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>资金融通助力下的“一带一路”硕果</td>\n",
       "      <td>中国经济周刊</td>\n",
       "      <td>2023-10-31 15:48:00_中国经济周刊</td>\n",
       "      <td>“消费”</td>\n",
       "      <td>0.597310</td>\n",
       "      <td>此外，中国已与20个共建国家签署双边本币互换协议，在17个共建国家建立人民币清算安排，人民币...</td>\n",
       "      <td>协议/协定(agreement):0.4,贸易/交易(trade):0.0</td>\n",
       "      <td>agreement:0.4, trade:0.0</td>\n",
       "      <td>{\"情绪得分\": \"0.5\"}</td>\n",
       "      <td>好的，我现在需要分析用户提供的新闻文本的市场情绪，并给出五档制的评分。首先，我要仔细阅读新闻...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要分析用户提供的新闻文本的市场情绪，并给出五档制的评分。首先...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11064</th>\n",
       "      <td>5000亿机票市场变局 票代夹缝求生</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>2017-10-12 00:00:00_第一财经日报</td>\n",
       "      <td>“消费”</td>\n",
       "      <td>0.610276</td>\n",
       "      <td>“赚快钱的时代结束了，投机取巧的窟窿会被一个个堵死，静下心来做市场、做产品、做服务才是唯一出...</td>\n",
       "      <td>盈利能力/获利能力(profitability):0.6,有利可图的/盈利的(profita...</td>\n",
       "      <td>profitability:0.6, profitable:0.8</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好，我来仔细分析一下这个新闻文本。首先，新闻内容是：“赚快钱的时代结束了，投机取巧的窟窿会被...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"好，我来仔细分析一下这个新闻文本。首先，新闻内容是：“赚快钱的时代结束了，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11065 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title NewsSource                       stratum  \\\n",
       "0                    下半场还是熊来了 牛市底盘裂缝      经济观察报     2008-03-17 08:23:00_经济观察报   \n",
       "1                            风投的另类生意       经济日报      2014-05-02 00:00:00_经济日报   \n",
       "2             投资者结构分化中的贵州茅台：千元股价贵不贵？     第一财经日报    2019-06-24 00:00:00_第一财经日报   \n",
       "3            32次降价失败后 购销“两票制”能让药价瘦身吗     第一财经日报    2017-01-10 00:00:00_第一财经日报   \n",
       "4                         用实验探索贫穷的本质      经济观察报     2019-10-19 02:09:00_经济观察报   \n",
       "...                              ...        ...                           ...   \n",
       "11060  退市新规精准发力：有效出清绩差风险公司 ST股数量总体稳定   21世纪经济报道  2024-06-06 00:00:00_21世纪经济报道   \n",
       "11061            提升电气化水平 驱动经济社会高质量发展      经济参考报     2020-06-15 00:00:00_经济参考报   \n",
       "11062                   “浑水”如何搅起一池浑水     第一财经日报    2011-11-25 00:00:00_第一财经日报   \n",
       "11063               资金融通助力下的“一带一路”硕果     中国经济周刊    2023-10-31 15:48:00_中国经济周刊   \n",
       "11064             5000亿机票市场变局 票代夹缝求生     第一财经日报    2017-10-12 00:00:00_第一财经日报   \n",
       "\n",
       "        topic  similarity                                           cut_news  \\\n",
       "0        “金融”    0.480989  易方达的一位基金经理表示，机构投资者在较早时期便开始减仓，目前市场更多表现为中小投资者的恐慌...   \n",
       "1        “金融”    0.491875                  “风投采取PIPE投资能增加PE资本流动性，提高PE资本的变现能力   \n",
       "2        “金融”    0.524268                                     ”深圳一位公募基金经理也表示   \n",
       "3      “生产投资”    0.474199                          这结果值得我们反思，根源是否就在于招采的制度和机制   \n",
       "4      “宏观经济”    0.505570                                    这种人民币形态的热钱规模有多大   \n",
       "...       ...         ...                                                ...   \n",
       "11060    “金融”    0.594900  同时，坚持对包括退市公司在内的财务造假公司及相关责任人违法违规行为“一追到底”、依法严惩，今...   \n",
       "11061  “生产投资”    0.488712      在城乡居民用电领域，加大北方地区清洁取暖工作力度，提升清洁能源供热能力，发挥电供暖示范作用   \n",
       "11062    “金融”    0.544560  业内人士透露，集体诉讼原告费用一般由律师预支，采取风险代理模式，如果诉讼成功，律师事务所能够...   \n",
       "11063    “消费”    0.597310  此外，中国已与20个共建国家签署双边本币互换协议，在17个共建国家建立人民币清算安排，人民币...   \n",
       "11064    “消费”    0.610276  “赚快钱的时代结束了，投机取巧的窟窿会被一个个堵死，静下心来做市场、做产品、做服务才是唯一出...   \n",
       "\n",
       "                                          sentiment_dict  \\\n",
       "0           撤资/抛售(divestment):-0.2,清仓/抛售(closeouts):-0.2   \n",
       "1              资本/资金(capital):0.1,投资/投资额(investment):0.4   \n",
       "2                        基金/资金(fund):0.1,基础/底部(base):0.0   \n",
       "3      选矿/精选(beneficiation):0.4,管理不善/管理失误(mismanageme...   \n",
       "4                    美分/分币(cent):0.0,货币/通货(currency):0.0   \n",
       "...                                                  ...   \n",
       "11060  应受惩罚的/可惩处的(punishable):-0.4,公司的/企业的(corporate)...   \n",
       "11061            北方/北部(north):0.0,电的/电气的(electrical):0.0   \n",
       "11062            诉讼当事人(litigant):0.0,诉讼/官司(lawsuit):-0.2   \n",
       "11063              协议/协定(agreement):0.4,贸易/交易(trade):0.0   \n",
       "11064  盈利能力/获利能力(profitability):0.6,有利可图的/盈利的(profita...   \n",
       "\n",
       "                           sentiment_dict_v2          Response  \\\n",
       "0            divestment:-0.2, closeouts:-0.2    {\"情绪得分\": \"-1\"}   \n",
       "1                capital:0.1, investment:0.4   {\"情绪得分\": \"0.5\"}   \n",
       "2                         fund:0.1, base:0.0     {\"情绪得分\": \"0\"}   \n",
       "3      beneficiation:0.4, mismanagement:-0.5  {\"情绪得分\": \"-0.5\"}   \n",
       "4                     cent:0.0, currency:0.0     {\"情绪得分\": \"0\"}   \n",
       "...                                      ...               ...   \n",
       "11060         punishable:-0.4, corporate:0.0     {\"情绪得分\": \"0\"}   \n",
       "11061              north:0.0, electrical:0.0     {\"情绪得分\": \"0\"}   \n",
       "11062             litigant:0.0, lawsuit:-0.2  {\"情绪得分\": \"-0.5\"}   \n",
       "11063               agreement:0.4, trade:0.0   {\"情绪得分\": \"0.5\"}   \n",
       "11064      profitability:0.6, profitable:0.8     {\"情绪得分\": \"0\"}   \n",
       "\n",
       "                                             Complex_CoT  score  \\\n",
       "0      好，我需要分析这篇新闻的情绪。新闻中提到易方达的基金经理说机构投资者早前就开始减仓，现在市场...   -1.0   \n",
       "1      好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“风投采取PIPE投资能增加PE资本流...    0.5   \n",
       "2      好的，我现在需要分析给定的新闻文本的情绪得分。首先，新闻文本是：“深圳一位公募基金经理也表示...    0.0   \n",
       "3      好的，我需要分析这条新闻的情绪得分。首先看一下新闻内容：“这结果值得我们反思，根源是否就在于...   -0.5   \n",
       "4      好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“这种人民币形态的热钱规模有多大。”这...    0.0   \n",
       "...                                                  ...    ...   \n",
       "11060  好的，我现在需要分析这段中文新闻的情绪，并给出五档制评分。首先，先仔细阅读新闻内容。\\n\\n...    0.0   \n",
       "11061  好的，我需要分析这段关于中国城乡居民用电领域的新闻文本的情绪，并根据提供的评分标准给出五档制...    0.0   \n",
       "11062  好，我需要分析这篇新闻的情绪，然后根据给定的情绪词典和评分标准来打分。首先，仔细阅读新闻内容...   -0.5   \n",
       "11063  好的，我现在需要分析用户提供的新闻文本的市场情绪，并给出五档制的评分。首先，我要仔细阅读新闻...    0.5   \n",
       "11064  好，我来仔细分析一下这个新闻文本。首先，新闻内容是：“赚快钱的时代结束了，投机取巧的窟窿会被...    0.0   \n",
       "\n",
       "                                                Question  \\\n",
       "0      **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "1      **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "2      **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "3      **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "4      **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "...                                                  ...   \n",
       "11060  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "11061  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "11062  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "11063  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "11064  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "\n",
       "                                              qwen_input  final_score_clean  \\\n",
       "0      **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...               -1.0   \n",
       "1      **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.5   \n",
       "2      **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.0   \n",
       "3      **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...               -0.5   \n",
       "4      **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.0   \n",
       "...                                                  ...                ...   \n",
       "11060  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.0   \n",
       "11061  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.5   \n",
       "11062  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.0   \n",
       "11063  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                1.0   \n",
       "11064  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...                0.5   \n",
       "\n",
       "                                             qwen_output  \n",
       "0      {\"情绪分析\":\"好，我需要分析这篇新闻的情绪。新闻中提到易方达的基金经理说机构投资者早前就...  \n",
       "1      {\"情绪分析\":\"好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“风投采取PIPE投...  \n",
       "2      {\"情绪分析\":\"好的，我现在需要分析给定的新闻文本的情绪得分。首先，新闻文本是：“深圳一位...  \n",
       "3      {\"情绪分析\":\"好的，我需要分析这条新闻的情绪得分。首先看一下新闻内容：“这结果值得我们反...  \n",
       "4      {\"情绪分析\":\"好的，我来分析一下这个新闻文本的情绪。首先，新闻内容是“这种人民币形态的热...  \n",
       "...                                                  ...  \n",
       "11060  {\"情绪分析\":\"好的，我现在需要分析这段中文新闻的情绪，并给出五档制评分。首先，先仔细阅读...  \n",
       "11061  {\"情绪分析\":\"好的，我需要分析这段关于中国城乡居民用电领域的新闻文本的情绪，并根据提供的...  \n",
       "11062  {\"情绪分析\":\"好，我需要分析这篇新闻的情绪，然后根据给定的情绪词典和评分标准来打分。首先...  \n",
       "11063  {\"情绪分析\":\"好的，我现在需要分析用户提供的新闻文本的市场情绪，并给出五档制的评分。首先...  \n",
       "11064  {\"情绪分析\":\"好，我来仔细分析一下这个新闻文本。首先，新闻内容是：“赚快钱的时代结束了，...  \n",
       "\n",
       "[11065 rows x 15 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新列 prompt_qwen，进行替换操作\n",
    "train['qwen_output'] = train.apply(lambda row: '{\"情绪分析\":\"'+ str(row['Complex_CoT']).replace('\"', '\\\\\"') + '\",\"情绪得分\":\"' + str((row['final_score_clean'])) + '\"}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[11064,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_scores = [-1, -0.5, 0, 0.5, 1,1.0,-0.50,-0.50,-1.0]\n",
    "\n",
    "# 筛选 score 列值在允许列表中的行\n",
    "train2 = train[train['final_score_clean'].isin(allowed_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train2.to_csv('/root/LLM_news_emo_analyze/DATA/qwen_sft_train_0405data/qwen_train_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11065\n",
      "11065\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0430 deepseek部分字段修改**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ds = \"\"\"**角色定义**\n",
    "擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评分，请思考后回答。\n",
    "\n",
    "**核心任务**\n",
    "1. 分析新闻整体内容情绪\n",
    "2. 匹配情绪词典关键词，对情绪进行深入理解与思考\n",
    "3. 思考后直接输出对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0），这些值分别代表非常消极、比较消极、中性、比较积极、非常积极）\n",
    "\n",
    "**评分逻辑**\n",
    "1. 语义匹配：忽略偏离词汇，保留有效情绪词\n",
    "2. 评分验证：整体语义优先，情绪词辅助修正\n",
    "\n",
    "**示例说明**\n",
    "- 示例1：\n",
    "新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\n",
    "情绪词典：profitability:0.6,profit:0.8\n",
    "输出:<think>语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破。“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1的评分逻辑。</think> {\"情绪得分\":\"1.0\"}\n",
    "\n",
    "- 示例2：\n",
    "新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\n",
    "情绪词典：mispricing:-0.4,advantage:0.7\n",
    "输出:<think>语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号。此外，新闻文本未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。整体乐观基调符合中档评分0.5，缺乏词典强匹配项限制进一步上调空间。</think> {\"情绪得分\":\"0.5\"}\n",
    "\n",
    "- 示例3：\n",
    "新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\n",
    "情绪词典：free:0.2,immateriality:-0.2\n",
    "输出:<think>语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响。此外，新闻文本中“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0。</think> {\"情绪得分\":\"0.0\"}\n",
    "\n",
    "- 示例4：\n",
    "新闻文本：我们投入的前期费用谁来承担\n",
    "情绪词典：invest:0.3\n",
    "输出:<think>语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪。新闻文本中“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位-0.5。</think> {\"情绪得分\":\"-0.5\"}\n",
    "\n",
    "- 示例5：\n",
    "新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\n",
    "情绪词典：crash:-0.9,meltdown:-0.8\n",
    "输出:<think>语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词验证：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，符合-1的情绪评分。</think> {\"情绪得分\":\"-1.0\"}\n",
    "\n",
    "**输出规范**\n",
    "- 情绪词典的分值仅作语义方向参考\n",
    "- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分值\n",
    "- 输出格式：{\"情绪得分\":\"...\"}\n",
    "\n",
    "现在请你开始进行分析，严格按照要求输出结果：\n",
    "新闻文本：{cut_news}\n",
    "情绪词典：{sentiment_dict_v2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Question'] = train.apply(\n",
    "    lambda row: prompt_ds.replace('{cut_news}', row['cut_news'] + ('。' if not row['cut_news'].endswith('。') else ''))\n",
    "              .replace('{sentiment_dict_v2}', row['sentiment_dict_v2']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test['Question'] = test.apply(\n",
    "    lambda row: prompt_ds.replace('{cut_news}', row['cut_news'] + ('。' if not row['cut_news'].endswith('。') else ''))\n",
    "              .replace('{sentiment_dict_v2}', row['sentiment_dict_v2']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评分，请思考后回答。\\n\\n**核心任务**\\n1. 分析新闻整体内容情绪\\n2. 匹配情绪词典关键词，对情绪进行深入理解与思考\\n3. 思考后直接输出对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0），这些值分别代表非常消极、比较消极、中性、比较积极、非常积极）\\n\\n**评分逻辑**\\n1. 语义匹配：忽略偏离词汇，保留有效情绪词\\n2. 评分验证：整体语义优先，情绪词辅助修正\\n\\n**示例说明**\\n- 示例1：\\n新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\\n情绪词典：profitability:0.6,profit:0.8\\n输出:<think>语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破。“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1的评分逻辑。</think> {\"情绪得分\":\"1.0\"}\\n\\n- 示例2：\\n新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\\n情绪词典：mispricing:-0.4,advantage:0.7\\n输出:<think>语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号。此外，新闻文本未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。整体乐观基调符合中档评分0.5，缺乏词典强匹配项限制进一步上调空间。</think> {\"情绪得分\":\"0.5\"}\\n\\n- 示例3：\\n新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\\n情绪词典：free:0.2,immateriality:-0.2\\n输出:<think>语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响。此外，新闻文本中“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0。</think> {\"情绪得分\":\"0.0\"}\\n\\n- 示例4：\\n新闻文本：我们投入的前期费用谁来承担\\n情绪词典：invest:0.3\\n输出:<think>语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪。新闻文本中“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位-0.5。</think> {\"情绪得分\":\"-0.5\"}\\n\\n- 示例5：\\n新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\\n情绪词典：crash:-0.9,meltdown:-0.8\\n输出:<think>语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词验证：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，符合-1的情绪评分。</think> {\"情绪得分\":\"-1.0\"}\\n\\n**输出规范**\\n- 情绪词典的分值仅作语义方向参考\\n- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分值\\n- 输出格式：{\"情绪得分\":\"...\"}\\n\\n现在请你开始进行分析，严格按照要求输出结果：\\n新闻文本：而福喜集团总部所在地的美国，更被称作是全球食品安全监管体系最完善的国家之一，其着力于事前监管，把对食品安全的监管起点推进到田间和生产商，并且，由中央财政统一拨款，独立的检验运作核算功能，使不法企业难以蠢蠢欲动。\\n情绪词典：perfect:1.0, food:0.0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11065\n",
      "JSON 数据集已成功保存到 /root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/qwen_train_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "def create_json_dataset(train_origin_data):\n",
    "    df = train_origin_data\n",
    "    dataset = []\n",
    "    for _, row in df.iterrows():\n",
    "        item = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": row['qwen_input']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": row['qwen_output']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        dataset.append(item)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_to_json(dataset, output_path):\n",
    "    # 获取文件所在的目录\n",
    "    dir_path = os.path.dirname(output_path)\n",
    "    # 检查目录是否存在，如果不存在则创建\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_json_path = '/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/qwen_train_data.json'  # 请替换为实际的输出 JSON 文件路径\n",
    "\n",
    "    json_dataset = create_json_dataset(train)\n",
    "    save_to_json(json_dataset, output_json_path)\n",
    "    print(f\"JSON 数据集已成功保存到 {output_json_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各分数的数量分布：\n",
      "final_score_clean\n",
      " 0.0    4499\n",
      " 0.5    2754\n",
      "-0.5    2001\n",
      " 1.0    1160\n",
      "-1.0     651\n",
      "Name: count, dtype: int64\n",
      "\n",
      "各分数的占比分布：\n",
      "final_score_clean\n",
      " 0.0    40.66%\n",
      " 0.5    24.89%\n",
      "-0.5    18.08%\n",
      " 1.0    10.48%\n",
      "-1.0     5.88%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 计数分布\n",
    "print(\"各分数的数量分布：\")\n",
    "print(train['final_score_clean'].value_counts(dropna=False))\n",
    "\n",
    "# 占比分布（百分比）\n",
    "print(\"\\n各分数的占比分布：\")\n",
    "print(train['final_score_clean'].value_counts(normalize=True, dropna=False).apply(lambda x: f\"{x:.2%}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_train_data.csv',encoding = 'utf_8_sig',index = False)\n",
    "test.to_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_test_data.csv',encoding = 'utf_8_sig',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **模型合并**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    " \n",
    "def apply_lora(model_name_or_path, output_path, lora_path):\n",
    "    print(f\"Loading the base model from {model_name_or_path}\")\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False, trust_remote_code=True)\n",
    "    base = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"cuda:0\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    # base.generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    print(f\"Loading the LoRA adapter from {lora_path}\")\n",
    " \n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        lora_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    " \n",
    "    print(\"Applying the LoRA\")\n",
    "    model = lora_model.merge_and_unload()\n",
    " \n",
    "    print(f\"Saving the target model to {output_path}\")\n",
    "    model.save_pretrained(output_path)\n",
    "    base_tokenizer.save_pretrained(output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lora_path = \"/root/LLM_news_emo_analyze/Qwen_output/0430_lora_model/checkpoint-2800\"\n",
    "    model_path = \"/root/.cache/LLMS/hub/Qwen/Qwen2___5-1___5B-Instruct\"\n",
    "    output = \"/root/LLM_news_emo_analyze/Qwen_output/0430_merge_model\"\n",
    "\n",
    "    apply_lora(model_path,output,lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **单个样本测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 模型路径配置\n",
    "#model_name = \"/root/.cache/LLMS/hub/Qwen/sft_model/checkpoint-2000\"\n",
    "model_name = \"/root/LLM_news_emo_analyze/Qwen_output/0430_merge_model\"\n",
    "\n",
    "# 初始化模型和分词器（优化后的加载方式）\n",
    "try:\n",
    "    # 加载分词器时显式指定padding方向\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        padding_side='left',  # 确保生成式任务的填充方向正确\n",
    "        trust_remote_code=True  # 网页12、17提到需要信任远程代码\n",
    "    )\n",
    "    \n",
    "    # 自动选择计算精度和设备分布\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",  # 网页1、4、12建议使用auto设备映射\n",
    "        max_memory={i: '20GB' for i in range(torch.cuda.device_count())},  # 显存优化（网页5、8）\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"初始化失败: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "def generate_response(prompt, max_length=512, temperature=0.4):\n",
    "    \"\"\"优化后的生成函数（参考网页12、18）\"\"\"\n",
    "    try:\n",
    "        # 构建消息模板（网页13、14的ChatML格式）\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # 应用聊天模板（网页12、14最佳实践）\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True  # 添加<|im_start|>assistant\\n前缀\n",
    "        )\n",
    "        \n",
    "        model_inputs = tokenizer(\n",
    "            [text],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(model.device)\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id  # 显式设置pad_token（网页8、9）\n",
    "        }\n",
    "        \n",
    "        # 执行生成\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            **generation_config\n",
    "        )\n",
    "        \n",
    "        response_ids = [output_ids[len(input_ids):] \n",
    "                       for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        \n",
    "        return tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"生成失败: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"**角色定义**\n",
    "你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的新闻文本情绪进行准确的评分：\n",
    "\n",
    "**任务流程**\n",
    "1. 分析新闻整体内容情绪\n",
    "2. 匹配情绪词典关键词，对情绪进行深入理解与分析\n",
    "3. 输出情绪分析跟对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0，分别代表非常消极、比较消极、中性、比较积极、非常积极）\n",
    "\n",
    "**评分逻辑**\n",
    "1. 语义匹配：忽略偏离的情绪词词汇，保留有效情绪词\n",
    "2. 评分调整：整体语义优先，情绪词辅助修正\n",
    "\n",
    "**示例说明**\n",
    "- 示例1：\n",
    "新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\n",
    "情绪词典：profitability:0.6,profit:0.8\n",
    "情绪分析:①语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破；②关键情绪词调整：“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1.0分（非常积极）的评分结果。\n",
    "情绪得分:1.0\n",
    "\n",
    "- 示例2：\n",
    "新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\n",
    "情绪词典：mispricing:-0.4,advantage:0.7\n",
    "情绪分析:①语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号；②关键情绪词调整：未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。由于缺乏词典强匹配项限制进一步上调空间，整体乐观基调符合“比较积极”，情绪得分0.5。\n",
    "情绪得分:0.5\n",
    "\n",
    "- 示例3：\n",
    "新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\n",
    "情绪词典：free:0.2,immateriality:-0.2\n",
    "情绪分析:①语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响；②关键情绪词调整：“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0.0分。\n",
    "情绪得分:0.0\n",
    "\n",
    "- 示例4：\n",
    "新闻文本：我们投入的前期费用谁来承担\n",
    "情绪词典：invest:0.3\n",
    "情绪分析:①语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪；②关键情绪词调整：“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位“比较消极”，即情绪得分-0.5分。\n",
    "情绪得分:-0.5\n",
    "\n",
    "- 示例5：\n",
    "新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\n",
    "情绪词典：crash:-0.9,meltdown:-0.8\n",
    "情绪分析:①语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词调整：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，因此是非常消极，情绪得分-1.0分。\n",
    "情绪得分:-1.0\n",
    "\n",
    "\n",
    "**其他说明**\n",
    "- 情绪词典的分值仅作语义方向参考\n",
    "- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分数\n",
    "- 输出格式：{\"情绪分析\":\"...\",\"情绪得分\":\"...\"}\n",
    "\n",
    "现在，请你开始分析并按照要求输出结果：\n",
    "新闻文本：发展大飞机对国民经济增长具有带动效应，大飞机产业链蓄势待发\n",
    "情绪词典：growth:0.7, productive:0.7\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪得分。首先，用户给了一个中文新闻文本，并且有一个情绪词典，里面有两个关键词：growth和productive，都是0.7分。\\n\\n首先，我得仔细阅读新闻内容：“发展大飞机对国民经济增长具有带动效应，大飞机产业链蓄势待发。” 这句话的关键点在于“带动效应”和“蓄势待发”。这里的“带动效应”明显是一个正面的描述，说明大飞机的发展会促进经济增长，而“蓄势待发”则暗示着未来会有更多的进展和机会，属于积极的信号。\\n\\n接下来要检查情绪词典里的关键词是否出现在新闻中。用户提供的情绪词典里有growth和productive，这两个词在新闻中确实出现了，原文中的“经济增长”对应了growth，“蓄势待发”可能没有直接出现productive这个词，但“蓄势待发”可以视为类似的意思，即准备充分，等待时机，这可能与productive相关，不过需要看是否有更直接的匹配。根据示例中的情况，比如示例2中的“便宜”虽然没直接匹配mispricing，但通过隐含意义进行了映射，所以这里可能类似，但需要确认是否严格按词典匹配。\\n\\n然后按照评分逻辑，首先是语义匹配，忽略不相关的词汇，只保留有效情绪词。这里的关键词是growth和productive，两个都是积极的，而且都对应到新闻中的词汇。“带动效应”中的“增长”对应growth，“蓄势待发”可能部分匹配productive，但productive更多指生产效率高，而这里的大飞机产业链蓄势待发更多是说准备好了，可能还没有实际产出，所以可能不算完全匹配。\\n\\n接下来是评分验证，整体语义优先。整个句子都在强调大飞机带来的积极影响，无论是经济增长还是产业链的蓄势待发，都是明显的正面消息。情绪词的存在加强了这一点，尤其是growth的0.7分，属于较高的积极分数。\\n\\n再看看提供的示例，例如示例1中，净利润大幅增长，直接匹配到了profit和profitability，得到了1分。而当前案例中的关键词虽然匹配到了，但可能因为不是完全相同的词，或者程度不如示例1那么强烈。例如，这里的“带动效应”可能比单纯的利润增长更积极，但示例2中的情况是间接映射，得到0.5分。但在这个例子中，直接提到了growth，而且整体语义非常积极，可能应该更高。\\n\\n不过需要注意的是，情绪词典中的growth是0.7，而productive是0.7，两者都是积极的，但可能存在重复的情况。如果两个词都被匹配到，可能会增强积极情绪。但根据示例中的处理方式，比如示例2中的情绪词并没有被直接匹配，但通过隐含意义调整了评分。因此，这里可能需要考虑是否“蓄势待发”是否足够匹配productive，或者是否只是部分相关。\\n\\n综合来看，整体语义非常积极，强调大飞机带来的经济增长和产业链的潜力，情绪词匹配到两个积极的方向，尽管可能没有完全相同的词汇，但整体语义足够强烈，应该评为1分。不过要看是否有足够的理由支持更高的分数。例如，在示例1中，净利润增长和盈利规模扩大，直接匹配了两个积极词，所以得了1分。而当前的例子中，虽然没有直接提到盈利，但“带动效应”和“蓄势待发”同样积极，可能属于同一档次，因此得分可能是1分。\\r\",\"情绪得分\":\"1.0\"}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **批量样本测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_test_data.csv\", encoding='utf_8_sig', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/news_emo/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-03 16:49:38,171 - INFO - 正在读取CSV文件: /root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/text_data_1000.xlsx\n",
      "2025-05-03 16:49:38,174 - ERROR - 处理CSV文件失败: 'utf-8' codec can't decode byte 0x87 in position 10: invalid start byte\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x87 in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24764/1908150028.py\u001b[0m in \u001b[0;36m<cell line: 270>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/text_data_1000.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     processed_df = process_csv_file(\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24764/1908150028.py\u001b[0m in \u001b[0;36mprocess_csv_file\u001b[0;34m(csv_path, output_path, input_column, output_column, batch_size, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# 读取CSV文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"正在读取CSV文件: {csv_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# 检查输入列是否存在\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1680\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x87 in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Union, Optional\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 从环境变量或默认值获取模型路径\n",
    "DEFAULT_MODEL_PATH = \"/root/LLM_news_emo_analyze/Qwen_output/0430_merge_model_2100\"\n",
    "MODEL_PATH = os.environ.get(\"QWEN_MODEL_PATH\", DEFAULT_MODEL_PATH)\n",
    "\n",
    "class QwenInferenceEngine:\n",
    "    \"\"\"Qwen模型推理引擎，支持批量处理\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = MODEL_PATH):\n",
    "        \"\"\"初始化模型和分词器\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_path,\n",
    "                padding_side='left',\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # 检测可用GPU并设置合适的精度\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            if self.device == \"cpu\":\n",
    "                logger.warning(\"未检测到GPU，将使用CPU进行推理，性能可能受限\")\n",
    "                torch_dtype = torch.float32\n",
    "            else:\n",
    "                logger.info(f\"检测到{torch.cuda.device_count()}个GPU设备\")\n",
    "                torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            \n",
    "            # 动态分配GPU内存\n",
    "            max_memory = {}\n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    gpu_total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB\n",
    "                    max_memory[i] = f\"{int(gpu_total_mem * 0.85)}GB\"  # 使用85%的GPU内存\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch_dtype,\n",
    "                device_map=\"auto\",\n",
    "                max_memory=max_memory if max_memory else None,\n",
    "                trust_remote_code=True\n",
    "            ).eval()\n",
    "            \n",
    "            # 验证模型加载状态\n",
    "            self._validate_model()\n",
    "            \n",
    "            logger.info(f\"成功加载Qwen模型，设备:{self.device}，精度:{torch_dtype}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型初始化失败: {str(e)}\")\n",
    "            raise RuntimeError(f\"模型初始化失败: {str(e)}\")\n",
    "    \n",
    "    def _validate_model(self):\n",
    "        \"\"\"验证模型是否正常加载和工作\"\"\"\n",
    "        try:\n",
    "            # 尝试进行一次简单推理\n",
    "            test_input = \"Hello\"\n",
    "            input_ids = self.tokenizer(test_input, return_tensors=\"pt\").input_ids.to(self.model.device)\n",
    "            self.model.generate(input_ids, max_new_tokens=5)\n",
    "            logger.info(\"模型验证通过\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型验证失败: {str(e)}\")\n",
    "            raise RuntimeError(f\"模型验证失败: {str(e)}\")\n",
    "    \n",
    "    def generate_response(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_new_tokens: int = 2048, \n",
    "        temperature: float = 0.4,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.1\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"单条输入的推理函数，保持向后兼容性\"\"\"\n",
    "        results = self.generate_batch([prompt], max_new_tokens, temperature, top_p, repetition_penalty)\n",
    "        return results[0] if results else None\n",
    "    \n",
    "    def generate_batch(\n",
    "        self, \n",
    "        prompts: List[str], \n",
    "        max_new_tokens: int = 2048, \n",
    "        temperature: float = 0.4,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.1,\n",
    "        batch_size: int = 15  # 控制单批次最大数量\n",
    "    ) -> List[Optional[str]]:\n",
    "        \"\"\"批量处理多个输入的推理函数\"\"\"\n",
    "        if not prompts:\n",
    "            logger.warning(\"收到空输入列表\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # 对较大的输入列表分批处理\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            batch_results = self._process_single_batch(\n",
    "                batch_prompts, \n",
    "                max_new_tokens, \n",
    "                temperature,\n",
    "                top_p,\n",
    "                repetition_penalty\n",
    "            )\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _process_single_batch(\n",
    "        self, \n",
    "        batch_prompts: List[str],\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        repetition_penalty: float\n",
    "    ) -> List[Optional[str]]:\n",
    "        \"\"\"处理单批次输入\"\"\"\n",
    "        try:\n",
    "            # 构建消息模板\n",
    "            batch_messages = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ] for prompt in batch_prompts\n",
    "            ]\n",
    "            \n",
    "            # 应用聊天模板\n",
    "            batch_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                ) for messages in batch_messages\n",
    "            ]\n",
    "            \n",
    "            # 统一处理tokenization\n",
    "            encodings = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,  # 控制输入最大长度\n",
    "                return_attention_mask=True\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # 保存输入长度用于截取输出\n",
    "            input_lengths = encodings.input_ids.shape[1]\n",
    "            \n",
    "            generation_config = {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"repetition_penalty\": repetition_penalty,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"attention_mask\": encodings.attention_mask,\n",
    "                \"do_sample\": temperature > 0.0,  # 温度大于0使用采样\n",
    "            }\n",
    "            \n",
    "            # 执行批量生成\n",
    "            with torch.no_grad():  # 优化内存使用\n",
    "                generated_ids = self.model.generate(\n",
    "                    encodings.input_ids,\n",
    "                    **generation_config\n",
    "                )\n",
    "            \n",
    "            # 分别提取每个输入对应的生成结果\n",
    "            batch_responses = []\n",
    "            for i, output_ids in enumerate(generated_ids):\n",
    "                # 提取生成的部分 (除去输入部分)\n",
    "                response_ids = output_ids[len(encodings.input_ids[i]):]\n",
    "                response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "                batch_responses.append(response_text)\n",
    "            \n",
    "            return batch_responses\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"批处理生成失败: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            # 返回对应数量的None，保持输出和输入一一对应\n",
    "            return [None] * len(batch_prompts)\n",
    "\n",
    "def process_csv_file(\n",
    "    csv_path: str, \n",
    "    output_path: str = None,\n",
    "    input_column: str = \"qwen_input\",\n",
    "    output_column: str = \"qwen_output\",\n",
    "    batch_size: int = 15,\n",
    "    max_new_tokens: int = 2048,\n",
    "    temperature: float = 0.4\n",
    "):\n",
    "    \"\"\"\n",
    "    批量处理CSV文件中的输入并将结果保存\n",
    "    \n",
    "    Args:\n",
    "        csv_path: CSV文件路径\n",
    "        output_path: 输出文件路径，默认为原文件名_processed.csv\n",
    "        input_column: 输入列名\n",
    "        output_column: 输出列名\n",
    "        batch_size: 批处理大小\n",
    "        max_new_tokens: 最大生成长度\n",
    "        temperature: 生成温度\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 设置默认输出路径\n",
    "        if output_path is None:\n",
    "            base_name = os.path.splitext(csv_path)[0]\n",
    "            output_path = f\"{base_name}_processed.csv\"\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        logger.info(f\"正在读取CSV文件: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # 检查输入列是否存在\n",
    "        if input_column not in df.columns:\n",
    "            raise ValueError(f\"输入列 '{input_column}' 在CSV文件中不存在\")\n",
    "        \n",
    "        # 提取输入列数据\n",
    "        input_texts = df[input_column].fillna(\"\").tolist()\n",
    "        total_rows = len(input_texts)\n",
    "        \n",
    "        logger.info(f\"开始处理 {total_rows} 行数据，批次大小: {batch_size}\")\n",
    "        \n",
    "        # 初始化模型\n",
    "        engine = QwenInferenceEngine()\n",
    "        \n",
    "        # 批量处理数据并显示进度\n",
    "        all_results = []\n",
    "        for i in tqdm(range(0, total_rows, batch_size), desc=\"批处理进度\"):\n",
    "            batch_texts = input_texts[i:i+batch_size]\n",
    "            batch_results = engine.generate_batch(\n",
    "                batch_texts, \n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # 定期保存中间结果，防止意外中断\n",
    "            if i > 0 and i % 3 == 0:\n",
    "                temp_df = df.copy()\n",
    "                temp_df[output_column] = all_results + [None] * (total_rows - len(all_results))\n",
    "                temp_df.to_csv(f\"{output_path}.temp\", index=False)\n",
    "                logger.info(f\"已保存中间结果 ({len(all_results)}/{total_rows})\")\n",
    "        \n",
    "        # 添加结果列\n",
    "        df[output_column] = all_results\n",
    "        \n",
    "        # 保存结果\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"处理完成，结果已保存至: {output_path}\")\n",
    "        \n",
    "        # 删除临时文件\n",
    "        if os.path.exists(f\"{output_path}.temp\"):\n",
    "            os.remove(f\"{output_path}.temp\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"处理CSV文件失败: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_test_data.csv\"\n",
    "    processed_df = process_csv_file(\n",
    "        csv_path=csv_path,\n",
    "        batch_size=15,\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.4\n",
    "    )\n",
    "    print(f\"成功处理 {len(processed_df)} 行数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/news_emo/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-03 19:13:51,594 - INFO - 正在读取CSV文件: /root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/data1000.xlsx\n",
      "2025-05-03 19:13:52,156 - INFO - CSV文件共有 1000 行，其中 1 行需要处理\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-05-03 19:13:53,058 - INFO - 检测到1个GPU设备\n",
      "2025-05-03 19:13:56,193 - INFO - 模型验证通过\n",
      "2025-05-03 19:13:56,316 - INFO - 成功加载Qwen模型，设备:cuda，精度:torch.bfloat16\n",
      "2025-05-03 19:13:56,317 - INFO - 开始处理 1 行空值数据，初始批次大小: 12\n",
      "2025-05-03 19:13:56,319 - INFO - 还需处理 1 行数据\n",
      "批处理进度: 100%|██████████| 1/1 [00:34<00:00, 34.14s/it]\n",
      "2025-05-03 19:14:30,666 - INFO - 处理完成，共处理 1/1 行，结果已保存至: /root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/data1000_processed.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成: 共 1000 行，已填充 1000 行，剩余空值 0 行\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from typing import List, Dict, Optional\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 模型默认路径\n",
    "DEFAULT_MODEL_PATH = \"/root/LLM_news_emo_analyze/Qwen_output/0430_merge_model_2100\"\n",
    "\n",
    "class QwenInferenceEngine:\n",
    "    \"\"\"Qwen模型推理引擎，支持批量处理和内存管理\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, config=None):\n",
    "        # 统一配置参数\n",
    "        self.config = {\n",
    "            \"max_new_tokens\": 3000,       # 最大生成长度\n",
    "            \"temperature\": 0.4,          # 温度参数\n",
    "            \"top_p\": 0.9,                # 采样参数\n",
    "            \"repetition_penalty\": 1.1,   # 重复惩罚系数\n",
    "            \"batch_size\": 15,             # 批处理大小\n",
    "            \"max_input_length\": 2048,    # 最大输入长度\n",
    "            \"clear_cuda_cache\": True,    # 是否清理CUDA缓存\n",
    "            \"dynamic_batch_size\": True,  # 是否启用动态批处理大小\n",
    "            \"min_batch_size\": 12,         # 最小批处理大小\n",
    "            \"cuda_empty_freq\": 1         # 每处理多少批次清空一次CUDA缓存\n",
    "        }\n",
    "        \n",
    "        # 更新用户提供的配置\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "            \n",
    "        # 设置模型路径\n",
    "        self.model_path = model_path or os.environ.get(\"QWEN_MODEL_PATH\", DEFAULT_MODEL_PATH)\n",
    "        \n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"加载模型和分词器\"\"\"\n",
    "        try:\n",
    "            # 先清理环境\n",
    "            self._clear_cuda_cache()\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_path,\n",
    "                padding_side='left',\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # 检测可用GPU并设置合适的精度\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            if self.device == \"cpu\":\n",
    "                logger.warning(\"未检测到GPU，将使用CPU进行推理，性能可能受限\")\n",
    "                torch_dtype = torch.float32\n",
    "            else:\n",
    "                logger.info(f\"检测到{torch.cuda.device_count()}个GPU设备\")\n",
    "                torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            \n",
    "            # 动态分配GPU内存\n",
    "            max_memory = {}\n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    gpu_total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB\n",
    "                    max_memory[i] = f\"{int(gpu_total_mem * 0.8)}GB\"  # 使用80%的GPU内存，留余量防止OOM\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch_dtype,\n",
    "                device_map=\"auto\",\n",
    "                max_memory=max_memory if max_memory else None,\n",
    "                trust_remote_code=True\n",
    "            ).eval()\n",
    "            \n",
    "            # 验证模型是否正常工作\n",
    "            self._validate_model()\n",
    "            \n",
    "            logger.info(f\"成功加载Qwen模型，设备:{self.device}，精度:{torch_dtype}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型初始化失败: {str(e)}\")\n",
    "            self._clear_cuda_cache()  # 失败时也清理缓存\n",
    "            raise RuntimeError(f\"模型初始化失败: {str(e)}\")\n",
    "    \n",
    "    def _validate_model(self):\n",
    "        \"\"\"验证模型是否正常加载和工作\"\"\"\n",
    "        try:\n",
    "            # 简单测试\n",
    "            test_input = \"Hello\"\n",
    "            input_ids = self.tokenizer(test_input, return_tensors=\"pt\").input_ids.to(self.model.device)\n",
    "            self.model.generate(input_ids, max_new_tokens=5)\n",
    "            logger.info(\"模型验证通过\")\n",
    "            \n",
    "            # 清理测试产生的缓存\n",
    "            self._clear_cuda_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型验证失败: {str(e)}\")\n",
    "            self._clear_cuda_cache()\n",
    "            raise RuntimeError(f\"模型验证失败: {str(e)}\")\n",
    "    \n",
    "    def _clear_cuda_cache(self):\n",
    "        \"\"\"清理CUDA缓存释放内存\"\"\"\n",
    "        if not torch.cuda.is_available() or not self.config[\"clear_cuda_cache\"]:\n",
    "            return\n",
    "            \n",
    "        # 确保所有GPU操作完成\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # 手动触发垃圾回收\n",
    "        gc.collect()\n",
    "        \n",
    "        # 清空CUDA缓存\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # 检查并报告当前内存使用情况\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                mem_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB\n",
    "                mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)  # GB\n",
    "                logger.debug(f\"GPU:{i} 分配内存:{mem_allocated:.2f}GB, 保留内存:{mem_reserved:.2f}GB\")\n",
    "    \n",
    "    def generate_response(self, prompt, **kwargs):\n",
    "        \"\"\"单条输入的推理函数，保持向后兼容性\"\"\"\n",
    "        results = self.generate_batch([prompt], **kwargs)\n",
    "        return results[0] if results else None\n",
    "    \n",
    "    def generate_batch(self, prompts, **kwargs):\n",
    "        \"\"\"批量处理多个输入的推理函数\"\"\"\n",
    "        if not prompts:\n",
    "            logger.warning(\"收到空输入列表\")\n",
    "            return []\n",
    "        \n",
    "        # 合并配置参数，允许临时覆盖默认配置\n",
    "        config = self.config.copy()\n",
    "        if kwargs:\n",
    "            config.update(kwargs)\n",
    "        \n",
    "        results = []\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        batch_counter = 0\n",
    "        \n",
    "        # 分批处理\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            \n",
    "            # 处理批次\n",
    "            try:\n",
    "                batch_results = self._process_single_batch(batch_prompts, config)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "                # 增加计数器\n",
    "                batch_counter += 1\n",
    "                \n",
    "                # 定期清理内存\n",
    "                if config[\"clear_cuda_cache\"] and batch_counter % config[\"cuda_empty_freq\"] == 0:\n",
    "                    logger.debug(f\"清理CUDA缓存（第{batch_counter}批次后）\")\n",
    "                    self._clear_cuda_cache()\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                # 处理CUDA内存不足错误\n",
    "                if \"CUDA out of memory\" in str(e) and config[\"dynamic_batch_size\"] and batch_size > config[\"min_batch_size\"]:\n",
    "                    # 清理内存\n",
    "                    self._clear_cuda_cache()\n",
    "                    \n",
    "                    # 减小批处理大小\n",
    "                    new_batch_size = max(batch_size // 2, config[\"min_batch_size\"])\n",
    "                    logger.warning(f\"CUDA内存不足，减小批处理大小: {batch_size} -> {new_batch_size}\")\n",
    "                    batch_size = new_batch_size\n",
    "                    config[\"batch_size\"] = new_batch_size\n",
    "                    \n",
    "                    # 重试当前批次\n",
    "                    i -= batch_size  # 回退以重新处理当前批次\n",
    "                    continue\n",
    "                else:\n",
    "                    # 其他错误，清理内存并重新抛出\n",
    "                    self._clear_cuda_cache()\n",
    "                    logger.error(f\"批处理错误: {str(e)}\")\n",
    "                    # 返回对应数量的None\n",
    "                    batch_results = [None] * len(batch_prompts)\n",
    "                    results.extend(batch_results)\n",
    "            \n",
    "        # 完成所有批次后清理内存\n",
    "        self._clear_cuda_cache()\n",
    "        return results\n",
    "    \n",
    "    def _process_single_batch(self, batch_prompts, config):\n",
    "        \"\"\"处理单批次输入\"\"\"\n",
    "        try:\n",
    "            # 构建消息模板\n",
    "            batch_messages = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ] for prompt in batch_prompts\n",
    "            ]\n",
    "            \n",
    "            # 应用聊天模板\n",
    "            batch_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                ) for messages in batch_messages\n",
    "            ]\n",
    "            \n",
    "            # 统一处理tokenization\n",
    "            encodings = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=config[\"max_input_length\"],\n",
    "                return_attention_mask=True\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # 生成配置\n",
    "            generation_config = {\n",
    "                \"max_new_tokens\": config[\"max_new_tokens\"],\n",
    "                \"temperature\": config[\"temperature\"],\n",
    "                \"top_p\": config[\"top_p\"],\n",
    "                \"repetition_penalty\": config[\"repetition_penalty\"],\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"attention_mask\": encodings.attention_mask,\n",
    "                \"do_sample\": config[\"temperature\"] > 0.0\n",
    "            }\n",
    "            \n",
    "            # 执行批量生成\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    encodings.input_ids,\n",
    "                    **generation_config\n",
    "                )\n",
    "            \n",
    "            # 提取生成结果\n",
    "            batch_responses = []\n",
    "            for i, output_ids in enumerate(generated_ids):\n",
    "                response_ids = output_ids[len(encodings.input_ids[i]):]\n",
    "                response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
    "                batch_responses.append(response_text)\n",
    "            \n",
    "            # 手动删除中间变量释放内存\n",
    "            del encodings\n",
    "            del generated_ids\n",
    "            \n",
    "            return batch_responses\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 转发错误以便上层处理OOM\n",
    "            raise\n",
    "        \n",
    "def process_csv_file(csv_path, output_path=None, input_column=\"qwen_input\", output_column=\"qwen_output\", **kwargs):\n",
    "    \"\"\"\n",
    "    批量处理CSV文件中的输入，只处理output_column为空的行\n",
    "    \n",
    "    Args:\n",
    "        csv_path: CSV文件路径\n",
    "        output_path: 输出文件路径，默认为原文件名_processed.csv\n",
    "        input_column: 输入列名\n",
    "        output_column: 输出列名\n",
    "        **kwargs: 可选的配置参数，会覆盖默认配置\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 设置默认输出路径\n",
    "        if output_path is None:\n",
    "            base_name = os.path.splitext(csv_path)[0]\n",
    "            output_path = f\"{base_name}_processed.csv\"\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        logger.info(f\"正在读取CSV文件: {csv_path}\")\n",
    "        # df = pd.read_csv(csv_path,encoding = 'utf_8_sig',lineterminator='\\n')\n",
    "        df = pd.read_excel(csv_path)\n",
    "        # df['qwen_score'] = None\n",
    "        # df['qwen_output'] = None\n",
    "        \n",
    "        # 检查输入列是否存在\n",
    "        if input_column not in df.columns:\n",
    "            raise ValueError(f\"输入列 '{input_column}' 在CSV文件中不存在\")\n",
    "        \n",
    "        # 确保输出列存在\n",
    "        if output_column not in df.columns:\n",
    "            df[output_column] = None\n",
    "        \n",
    "        # 判断哪些行需要处理（output_column为空的行）\n",
    "        empty_mask = df[output_column].isna() | (df[output_column] == \"\") | (df[output_column].astype(str) == \"nan\") | (df['qwen_score'].astype(str) == \"nan\")\n",
    "        rows_to_process = df[empty_mask].index.tolist()\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        empty_rows = len(rows_to_process)\n",
    "        \n",
    "        logger.info(f\"CSV文件共有 {total_rows} 行，其中 {empty_rows} 行需要处理\")\n",
    "        \n",
    "        # 如果没有需要处理的行，直接返回\n",
    "        if empty_rows == 0:\n",
    "            logger.info(\"没有空值行需要处理，直接返回原文件\")\n",
    "            return df\n",
    "        \n",
    "        # 只提取需要处理的行的输入\n",
    "        inputs_to_process = df.loc[rows_to_process, input_column].fillna(\"\").tolist()\n",
    "        \n",
    "        # 设置更加保守的默认配置以避免OOM\n",
    "        default_config = {\n",
    "            \"batch_size\": 12,                # 默认较小批次避免OOM\n",
    "            \"clear_cuda_cache\": True,       # 启用内存清理\n",
    "            \"dynamic_batch_size\": True,     # 启用动态批大小\n",
    "            \"cuda_empty_freq\": 1            # 每批次都清理\n",
    "        }\n",
    "        \n",
    "        # 合并用户配置\n",
    "        config = default_config.copy()\n",
    "        config.update(kwargs)\n",
    "        \n",
    "        # 初始化模型引擎\n",
    "        engine = QwenInferenceEngine(config=config)\n",
    "        batch_size = engine.config[\"batch_size\"]\n",
    "        \n",
    "        logger.info(f\"开始处理 {empty_rows} 行空值数据，初始批次大小: {batch_size}\")\n",
    "        \n",
    "        # 批量处理数据并显示进度\n",
    "        results_map = {}  # 使用字典存储结果，键为行索引\n",
    "        processed_count = 0\n",
    "        \n",
    "        # 尝试从临时文件恢复部分结果\n",
    "        if os.path.exists(f\"{output_path}.temp\"):\n",
    "            try:\n",
    "                temp_df = pd.read_csv(f\"{output_path}.temp\")\n",
    "                if output_column in temp_df.columns:\n",
    "                    # 只复制已处理的行\n",
    "                    for idx in rows_to_process:\n",
    "                        if idx < len(temp_df) and not pd.isna(temp_df.loc[idx, output_column]) and temp_df.loc[idx, output_column] != \"\":\n",
    "                            results_map[idx] = temp_df.loc[idx, output_column]\n",
    "                            processed_count += 1\n",
    "                    \n",
    "                    if processed_count > 0:\n",
    "                        logger.info(f\"从临时文件恢复了 {processed_count}/{empty_rows} 行结果\")\n",
    "            except Exception as temp_e:\n",
    "                logger.warning(f\"读取临时文件失败: {str(temp_e)}\")\n",
    "        \n",
    "        # 更新需要处理的行\n",
    "        rows_to_process = [idx for idx in rows_to_process if idx not in results_map]\n",
    "        inputs_to_process = df.loc[rows_to_process, input_column].fillna(\"\").tolist()\n",
    "        \n",
    "        logger.info(f\"还需处理 {len(rows_to_process)} 行数据\")\n",
    "        \n",
    "        try:\n",
    "            # 按批次处理\n",
    "            for i in tqdm(range(0, len(rows_to_process), batch_size), desc=\"批处理进度\"):\n",
    "                batch_indices = rows_to_process[i:i+batch_size]\n",
    "                batch_inputs = inputs_to_process[i:i+batch_size]\n",
    "                \n",
    "                # 处理一批数据\n",
    "                batch_results = engine.generate_batch(batch_inputs)\n",
    "                \n",
    "                # 将结果与原始行索引对应起来\n",
    "                for j, result in enumerate(batch_results):\n",
    "                    idx = batch_indices[j]\n",
    "                    results_map[idx] = result\n",
    "                \n",
    "                processed_count += len(batch_indices)\n",
    "                \n",
    "                # 定期保存中间结果\n",
    "                if i > 0 and i % 50 == 0:\n",
    "                    # 将已处理的结果更新到DataFrame\n",
    "                    temp_df = df.copy()\n",
    "                    for idx, result in results_map.items():\n",
    "                        temp_df.loc[idx, output_column] = result\n",
    "                    \n",
    "                    # 保存临时文件\n",
    "                    temp_df.to_csv(f\"{output_path}.temp\", index=False,encoding = 'utf_8_sig')\n",
    "                    logger.info(f\"已保存中间结果 ({processed_count}/{empty_rows} 行)\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\"用户中断处理，保存当前进度\")\n",
    "            \n",
    "        finally:\n",
    "            # 将所有结果更新到原始DataFrame\n",
    "            for idx, result in results_map.items():\n",
    "                df.loc[idx, output_column] = result\n",
    "            \n",
    "            # 保存最终结果\n",
    "            df.to_csv(output_path, index=False)\n",
    "            logger.info(f\"处理完成，共处理 {len(results_map)}/{empty_rows} 行，结果已保存至: {output_path}\")\n",
    "            \n",
    "            # 删除临时文件\n",
    "            if os.path.exists(f\"{output_path}.temp\"):\n",
    "                os.remove(f\"{output_path}.temp\")\n",
    "            \n",
    "            # 清理资源\n",
    "            engine._clear_cuda_cache()\n",
    "            del engine\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"处理CSV文件失败: {str(e)}\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        raise\n",
    "\n",
    "# 向后兼容的全局函数\n",
    "_global_engine = None\n",
    "\n",
    "def generate_response(prompt, max_length=512, temperature=0.5):\n",
    "    \"\"\"兼容原始API的推理函数\"\"\"\n",
    "    global _global_engine\n",
    "    if _global_engine is None:\n",
    "        _global_engine = QwenInferenceEngine()\n",
    "    return _global_engine.generate_response(prompt, max_new_tokens=max_length, temperature=temperature)\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/data1000.xlsx\"\n",
    "    \n",
    "    # 设置更保守的参数以避免OOM\n",
    "    config = {\n",
    "        \"max_new_tokens\": 4096,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 12,              # 初始批处理大小\n",
    "        \"clear_cuda_cache\": True,     # 启用内存清理\n",
    "        \"dynamic_batch_size\": True,   # 启用动态批处理大小调整\n",
    "        \"min_batch_size\": 12,          # 最小可降至12条处理\n",
    "        \"cuda_empty_freq\": 1          # 每批次后清理\n",
    "    }\n",
    "    \n",
    "    processed_df = process_csv_file(\n",
    "        csv_path=csv_path,\n",
    "        input_column=\"qwen_input\",\n",
    "        output_column=\"qwen_output\",\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # 输出处理统计\n",
    "    empty_after = processed_df[\"qwen_output\"].isna().sum()\n",
    "    filled = len(processed_df) - empty_after\n",
    "    print(f\"处理完成: 共 {len(processed_df)} 行，已填充 {filled} 行，剩余空值 {empty_after} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **qwen的score提取以及合理值检验**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/data1000_processed.csv',encoding = 'utf_8_sig',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_score_by_regex(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    # 匹配 \"情绪得分\":\"1.0\" 或 \"情绪得分\": \"0.5\" 等格式\n",
    "    match = re.search(r'\"情绪得分\"\\s*:\\s*\"?(?P<score>-?1\\.0|-?0\\.5|0\\.5|0\\.0|0|1\\.0|1)\"?', text)\n",
    "    if match:\n",
    "        return match.group(\"score\")\n",
    "    return None\n",
    "\n",
    "# 应用到 DataFrame\n",
    "test['qwen_score'] = test['qwen_output'].apply(extract_score_by_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有 qwen_score 的值都合法。\n"
     ]
    }
   ],
   "source": [
    "valid_scores = {-1.0, -0.5, 0.0, 0.5, 1.0}\n",
    "\n",
    "# 转换为 float，确保是数值型\n",
    "test['qwen_score'] = pd.to_numeric(test['qwen_score'], errors='coerce')\n",
    "\n",
    "# 检查是否存在非法得分\n",
    "invalid_scores = test[~test['qwen_score'].isin(valid_scores)]\n",
    "\n",
    "# 输出结果\n",
    "if invalid_scores.empty:\n",
    "    print(\"✅ 所有 qwen_score 的值都合法。\")\n",
    "else:\n",
    "    print(f\"❌ 存在非法得分，共 {len(invalid_scores)} 条记录：\")\n",
    "    print(invalid_scores[['qwen_output', 'qwen_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制评分。首先，我要仔细阅读用户提供的所有信息，确保自己完全理解任务的要求。\\n\\n首先看新闻文本：“通过国家医保谈判，4款国产药物的降价幅度高达80%以上。” 这句话的关键点在于国家医保谈判带来的药品降价效果显著，具体来说是“高达80%以上”。这里有两个主要部分：谈判过程以及降价幅度。接下来要结合提供的情绪词典来进行分析。\\n\\n情绪词典中有两个词：markdown:0.0 和 undercut:-0.3。我需要检查新闻文本中是否有这些关键词或者相关词汇。原文提到的是“降价幅度高达80%以上”，这里的“降价”可能对应到情绪词典里的“undercut”吗？不过情绪词典里并没有这个词，所以可能需要考虑其他因素。\\n\\n根据评分逻辑，首先要进行语义匹配，忽略不相关的词汇，只保留有效情绪词。然后整体语义优先，情绪词辅助修正。虽然情绪词典中没有直接匹配的词，但需要看整体内容是否带有明显的积极或消极情绪。\\n\\n新闻中的核心信息是国产药物通过医保谈判获得了大幅度降价，这通常被视为积极的消息，因为它降低了患者用药的成本，可能促进医疗可及性和公平性。然而，情绪词典中的“undercut”是指压价销售，而文中提到的是降价幅度高，可能并不等同于压价行为，因为降价本身是积极措施。但情绪词典里只有undercut:-0.3，所以如果这里确实出现了“降价”这样的词汇，那么可能需要考虑是否属于undercut的范畴。\\n\\n但用户提供的情绪词典中undercut的分值是-0.3，而markdown是0.0。在新闻文本中，有没有出现这两个词呢？原文中的“降价”是否对应undercut？可能不太直接，因为undercut更多指主动压价，而这里只是被动降价，所以可能不算undercut。这时候需要判断整体语义是否积极。\\n\\n根据示例中的情况，比如示例5中虽然没有直接匹配情绪词，但整体语义非常负面，所以评分为-1。同样，如果整体语义足够积极，即使没有直接的情绪词匹配，也可能评为较高分。例如示例1中，净利润大幅增长，直接匹配了profit和profitability，所以得了1分。而示例2中，虽然有间接映射，但因为没有直接匹配，所以得0.5分。\\n\\n在这个案例中，新闻内容明显是关于降价的积极消息，尤其是“高达80%以上”的降幅，这对消费者来说是利好。但情绪词典中没有对应的积极词汇，只有undercut:-0.3，而markdown是0.0。可能需要看是否“降价”可以对应到undercut。如果是的话，那情绪词会带来轻微的负面修正，但整体语义还是积极的。\\n\\n根据评分逻辑，整体语义优先。如果整体是积极的，即使情绪词典中没有直接匹配的积极词汇，也可以评为较高的分数。例如示例5中，虽然没有直接匹配情绪词，但整体语义非常负面，所以评了-1。同样，这里的情况是积极的，但情绪词典中没有积极的词汇，只有undercut的-0.3。这时候可能需要看是否整体语义足够强烈，即使没有情绪词的支持，也能评为0.5或更高？\\n\\n另外，用户提供的示例中，当有情绪词但整体语义积极时，如示例1，得到1分。而如果有情绪词但整体语义不够强，如示例2，得到0.5分。这里的情况是，新闻内容明显积极，但情绪词典中没有对应的积极词汇，只有undercut的-0.3。这时候可能需要考虑是否整体语义足够强，可以忽略情绪词典的限制，或者是否应该以情绪词作为辅助修正。\\n\\n根据示例5，虽然情绪词典中没有直接匹配，但整体语义足够负面，所以评了-1。类似地，如果整体语义足够积极，即使没有情绪词支持，也可以评为更高的分数。例如，示例1中，净利润增长直接对应profit和profitability，所以得1分。而这里，降价幅度大，可能属于较强的积极信号，但如果没有情绪词匹配，可能只能评为0.5分？\\n\\n或者，考虑到情绪词典中的undercut是-0.3，而降价幅度高可能暗示降价策略，但降价本身通常是积极的，所以可能整体语义是积极的，尽管情绪词典中没有对应的积极词汇。这种情况下，可能需要根据整体语义来决定评分。\\n\\n根据示例2，当有间接映射时，可能评为0.5分。而这里，如果整体语义足够积极，可能评为0.5分。但需要确认是否有足够的积极因素。例如，降价幅度如此之大，可能意味着产品竞争力强，企业利润下降，但最终消费者受益，属于积极的社会效益。这种情况下，可能视为积极消息，但情绪词典中没有对应的积极词汇，所以可能评为0.5分。\\n\\n或者，可能因为情绪词典中没有积极词汇，所以无法达到更高的分数。例如，示例2中，虽然有间接映射，但因为没有直接匹配，所以得0.5。而这里，如果整体语义积极，但情绪词典中没有积极词汇，可能只能评为0.5分。或者，如果认为降价幅度大本身就是强烈的积极信号，即使没有情绪词，也可以评为1分？\\n\\n需要再仔细看一下评分标准。评分逻辑中提到，整体语义优先，情绪词辅助修正。如果整体语义足够积极，即使没有情绪词，也可以评为1分。例如示例1中，净利润增长和盈利规模扩大，都是积极因素，且有情绪词匹配，所以得1分。而这里，新闻内容是降价幅度大，可能属于积极，但情绪词典中没有对应的积极词汇，所以可能只能评为0.5分？\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut在这里是负面的，所以可能需要将情绪词的影响反转。例如，如果降价幅度高，可能被视为undercut，但情绪词典中undercut是-0.3，所以这里可能算作负面情绪词，但整体语义是积极的，所以需要修正为正面。在这种情况下，整体语义积极，但情绪词典中没有积极词汇，所以可能评为0.5分？\\n\\n或者，可能因为整体语义足够积极，即使没有情绪词，也可以评为1分。例如，示例5中没有直接匹配情绪词，但整体语义非常负面，所以评了-1。同样的，如果整体语义非常积极，即使没有情绪词，也可以评为1分？\\n\\n需要对比示例。示例1中的情绪词是profit和profitability，都是积极的，所以得1分。而这里，情绪词典中的undercut是-0.3，但新闻中的降价可能对应undercut，但undercut是-0.3，所以可能整体语义是积极的，但情绪词是负面的，这时候如何处理？\\n\\n根据评分逻辑，语义匹配要忽略偏离的词汇，保留有效情绪词。也就是说，如果新闻中提到了降价，可能对应undercut，但undercut是-0.3，所以可能整体语义是积极的，但由于情绪词是负面的，所以需要修正。例如，在示例5中，虽然情绪词典中没有直接匹配，但整体语义负面，所以评了-1。同样，这里，整体语义积极，但情绪词典中的undercut是-0.3，所以可能整体语义是积极的，但情绪词是负面的，这时候需要修正为正面。\\n\\n在这种情况下，可能整体语义是积极的，但情绪词是负面的，所以整体语义优先，可能评为0.5分。例如，示例2中，虽然有间接映射，但得0.5分。而这里，整体语义积极，但情绪词是负面的，所以可能评为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候需要修正为正面。例如，如果整体语义是积极的，但情绪词是负面的，那么可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，可能因为情绪词典中的undercut是-0.3，而降价幅度高可能对应undercut，但undercut是-0.3，所以整体语义是积极的，但情绪词是负面的，这时候可能评为0.5分，因为整体语义是积极的，但情绪词是负面的，所以修正为0.5分。\\n\\n或者，'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[225,-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_excel('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0503/data1000_process.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_test_data_processed_222.csv',index = False, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **结果准确率评估**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "test = pd.read_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/wxy_change_label.csv',encoding = 'utf_8_sig',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ds_api的score提取\n",
    "\n",
    "def extract_sentiment_score(df, text_column, new_column):\n",
    "    # 正则表达式兼容两种引号情况\n",
    "    pattern = r'(?:\"情绪得分\":(?: \"|\")([-\\d.]+)(?:\"))'\n",
    "    df[new_column] = df[text_column].apply(\n",
    "        lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else None)\n",
    "    return df\n",
    "\n",
    "# 调用函数提取情绪得分\n",
    "test = extract_sentiment_score(test, 'content', 'ds_api_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>NewsSource</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>stratum</th>\n",
       "      <th>topic</th>\n",
       "      <th>similarity</th>\n",
       "      <th>prompt</th>\n",
       "      <th>cut_news</th>\n",
       "      <th>sentiment_dict</th>\n",
       "      <th>...</th>\n",
       "      <th>qwen_input</th>\n",
       "      <th>Response</th>\n",
       "      <th>Complex_CoT</th>\n",
       "      <th>label</th>\n",
       "      <th>Question</th>\n",
       "      <th>qwen_output</th>\n",
       "      <th>qwen_score</th>\n",
       "      <th>content</th>\n",
       "      <th>reasoning_content\\r</th>\n",
       "      <th>ds_api_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>评论：一份有情怀有思路的政府报告</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>15-Mar</td>\n",
       "      <td>2015-03-06 00:00:00_第一财经日报</td>\n",
       "      <td>“宏观经济”</td>\n",
       "      <td>0.543562</td>\n",
       "      <td>新闻文本是##“总理的报告，在解题思路方面亮点纷呈”##，包含的情绪词典是##：解决/解答(...</td>\n",
       "      <td>总理的报告，在解题思路方面亮点纷呈</td>\n",
       "      <td>解决/解答(solve):0.3,问题/难题(problem):-0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0.5\"}</td>\n",
       "      <td>好的，我现在需要分析这段新闻文本的情绪得分。首先，新闻内容是关于总理的报告在解题思路方面有亮...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪得分。首先，用...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息强调总理报告的解题思路存在'亮点纷呈'，突出其策略创新性与解决方...</td>\n",
       "      <td>好的，我现在需要分析这个新闻文本的情绪，并根据给定的情绪词典给出评分。首先，新闻文本是：“总...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>基金年度“成绩单”角逐进入白热化 绩优产品多布局传媒消费通讯等领域</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>23-Nov</td>\n",
       "      <td>2023-11-28 00:00:00_经济参考报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.501491</td>\n",
       "      <td>新闻文本是##“截至目前，共989只基金成立以来的回报率在100%以上”##，包含的情绪词典...</td>\n",
       "      <td>截至目前，共989只基金成立以来的回报率在100%以上</td>\n",
       "      <td>百分比(percentage):0.0,基金/资金(fund):0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0.5\"}</td>\n",
       "      <td>好的，我来仔细分析一下这个新闻文本的情绪。首先，新闻内容是：“截至目前，共989只基金成立以...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我需要分析这条新闻的情绪，并给出五档评分。首先看新闻内容：“截至目前...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为近千只基金成立以来的回报率超过100%，展示出较强的长期收益能...</td>\n",
       "      <td>好的，我需要分析这条新闻文本的情绪，并给出对应的五档制评分。首先，看看新闻内容：“截至目前，...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>元旦谁抢中国游客多？日本韩国，一个笑了一个哭了</td>\n",
       "      <td>每日经济新闻</td>\n",
       "      <td>18-Jan</td>\n",
       "      <td>2018-01-02 00:00:00_每日经济新闻</td>\n",
       "      <td>“消费”</td>\n",
       "      <td>0.611313</td>\n",
       "      <td>新闻文本是##“携程跟团游、自由行数据，香港、曼谷、东京、新加坡、大阪、长滩岛、台北、芽庄、...</td>\n",
       "      <td>携程跟团游、自由行数据，香港、曼谷、东京、新加坡、大阪、长滩岛、台北、芽庄、迪拜、伊斯坦布尔...</td>\n",
       "      <td>海外的/国外的(overseas):0.0,外国的/国外的(foreign):0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我现在需要分析这条新闻的情绪得分。首先，看看新闻内容。主要讲的是携程的跟团游和自由行数...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为列举中国游客境外热门旅游目的地及购物季对城市人气的提升效应，属...</td>\n",
       "      <td>好的，我现在需要分析这个新闻文本的情绪，并根据给定的任务流程和评分逻辑给出结果。首先，我要仔...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>又一大省放开落户！除两城外，山东其余14城或全部“零门槛”</td>\n",
       "      <td>21世纪经济报道</td>\n",
       "      <td>20-Jan</td>\n",
       "      <td>2020-01-13 00:00:00_21世纪经济报道</td>\n",
       "      <td>“房地产”</td>\n",
       "      <td>0.534588</td>\n",
       "      <td>新闻文本是##“生产要素包括人口的流动还会更加自由，所以城镇化绝对不是一个就地城镇化的概念”...</td>\n",
       "      <td>生产要素包括人口的流动还会更加自由，所以城镇化绝对不是一个就地城镇化的概念</td>\n",
       "      <td>自由的/免费的(free):0.2,非流动性/缺乏流动性(illiquidity):-0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好的，我现在需要分析给定的新闻文本的情绪得分。首先，看一下新闻内容：“生产要素包括人口的流动...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要分析给定的新闻文本的情绪，并根据提供的情绪词典给出五档制评...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息强调生产要素与人口流动将更加自由，并明确否定就地城镇化的局限性，...</td>\n",
       "      <td>好的，我现在需要分析这条新闻文本的情绪，并给出对应的五档制评分。首先，我得仔细阅读新闻内容，...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>专访顾云昌：北京共有产权房是1998年房改后的又一重大创新</td>\n",
       "      <td>中国经济周刊</td>\n",
       "      <td>17-Aug</td>\n",
       "      <td>2017-08-11 17:35:00_中国经济周刊</td>\n",
       "      <td>“房地产”</td>\n",
       "      <td>0.549985</td>\n",
       "      <td>新闻文本是##“而保障房可分为两种：公租房，以及新推出的共有产权房”##，包含的情绪词典是#...</td>\n",
       "      <td>而保障房可分为两种：公租房，以及新推出的共有产权房</td>\n",
       "      <td>房子/住宅(house):0.0,房地产/财产(estate):0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0\"}</td>\n",
       "      <td>好，我需要分析这段关于中国保障房的新闻文本的情绪。首先，看整体内容。新闻提到保障房分为公租房...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理用户提供的这个市场情绪分析任务。首先，我得仔细阅读用户...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为对保障房类型的客观分类说明，未体现政策效果或市场影响的价值判断...</td>\n",
       "      <td>好的，我现在要处理这条新闻文本的情绪分析任务。首先，我需要仔细阅读新闻内容：“而保障房可分为...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>492</td>\n",
       "      <td>赢创：创新环保及城镇化将成为今后发展三大主题</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>13-Apr</td>\n",
       "      <td>2013-04-03 00:00:00_第一财经日报</td>\n",
       "      <td>“生产投资”</td>\n",
       "      <td>0.482415</td>\n",
       "      <td>新闻文本是##“环保标准全球统一\\n　　研发新产品是一家创新型特种化学品企业的核心要素之一”...</td>\n",
       "      <td>环保标准全球统一\\n　　研发新产品是一家创新型特种化学品企业的核心要素之一</td>\n",
       "      <td>创新/革新(innovation):0.7,创新性/革新性(innovativeness):0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"0.5\"}</td>\n",
       "      <td>好的，我现在需要处理这个用户的查询。首先，用户给了一个任务，要求作为中英双语市场情绪分析专家...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为强调环保标准全球统一及创新型企业的研发重要性，前者隐含标准化带...</td>\n",
       "      <td>好，我现在要分析这个新闻文本的情绪得分。首先，新闻文本是：“环保标准全球统一 研发新产品是一...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>482</td>\n",
       "      <td>“七连涨”遇大跳水 融资盘企稳再受考验</td>\n",
       "      <td>第一财经日报</td>\n",
       "      <td>15-Aug</td>\n",
       "      <td>2015-08-19 00:00:00_第一财经日报</td>\n",
       "      <td>“金融”</td>\n",
       "      <td>0.546733</td>\n",
       "      <td>新闻文本是##“当8月18日大盘调整再度来袭时，资金面信心也将再次接受严峻考验”##，包含的...</td>\n",
       "      <td>当8月18日大盘调整再度来袭时，资金面信心也将再次接受严峻考验</td>\n",
       "      <td>资金不足的(underfunded):-0.3,衰退/不景气(recession):-0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"-0.5\"}</td>\n",
       "      <td>好的，我需要分析这条新闻的情绪，并给出五档评分。首先看新闻内容：“当8月18日大盘调整再度来...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要分析给定的新闻文本，并根据提供的情绪词典和评分规则给出情绪...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为大盘调整再度来袭导致资金面信心面临严峻考验，暗示市场波动加剧及...</td>\n",
       "      <td>好的，我现在需要分析用户提供的新闻文本，并按照给定的任务流程和评分逻辑来给出情绪分析和对应的...</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>498</td>\n",
       "      <td>​积极推进新型城镇化，各类要素如何双向流动</td>\n",
       "      <td>中国经济周刊</td>\n",
       "      <td>24-Apr</td>\n",
       "      <td>2024-04-30 15:37:00_中国经济周刊</td>\n",
       "      <td>“房地产”</td>\n",
       "      <td>0.634848</td>\n",
       "      <td>新闻文本是##“规划时，一些地方官员没有考虑到当地的具体情况，易导致出现不良资产，尤其是个别...</td>\n",
       "      <td>规划时，一些地方官员没有考虑到当地的具体情况，易导致出现不良资产，尤其是个别人口流出地区，还...</td>\n",
       "      <td>未充分利用(underutilization):-0.3,管理不善/错误管理(mismana...</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"-0.5\"}</td>\n",
       "      <td>好的，我现在需要分析这篇新闻文本的情绪得分。首先，新闻内容是关于地方官员在规划时没有考虑当地...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为地方官员规划忽视当地实情导致不良资产及人口流出地区资源过度投入...</td>\n",
       "      <td>好的，我来仔细分析一下这个新闻文本。首先，文本提到地方官员在规划时没有考虑当地具体情况，导致...</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>村里超市开业了</td>\n",
       "      <td>经济日报</td>\n",
       "      <td>7-Jan</td>\n",
       "      <td>2007-01-14 00:00:00_经济日报</td>\n",
       "      <td>“消费”</td>\n",
       "      <td>0.547639</td>\n",
       "      <td>新闻文本是##“现在农村有了超市，好多假货彻底失去了市场”##，包含的情绪词典是##：无销路...</td>\n",
       "      <td>现在农村有了超市，好多假货彻底失去了市场</td>\n",
       "      <td>无销路的/滞销的(unmarketable):-0.5,滞销(unsellable):-0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"1\"}</td>\n",
       "      <td>好的，我需要分析这条新闻的情绪。新闻内容是：“现在农村有了超市，好多假货彻底失去了市场。”首...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我需要分析这段新闻文本的情绪，并给出五档制评分。首先看新闻内容：“现...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为农村超市普及导致假货市场消失，反映消费环境优化与正品流通保障的...</td>\n",
       "      <td>好的，我来仔细分析一下这个新闻文本。首先，新闻内容是“现在农村有了超市，好多假货彻底失去了市...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>499</td>\n",
       "      <td>在中央政治局常委会会议研究应对新型冠状病毒肺炎疫情工作时的讲话</td>\n",
       "      <td>经济日报</td>\n",
       "      <td>20-Feb</td>\n",
       "      <td>2020-02-16 00:00:00_经济日报</td>\n",
       "      <td>“生产投资”</td>\n",
       "      <td>0.516709</td>\n",
       "      <td>新闻文本是##“以前，针对原材料价格低、企业生产规模小、铝型材巿场受塑钢冲击大的特点，把经营...</td>\n",
       "      <td>以前，针对原材料价格低、企业生产规模小、铝型材巿场受塑钢冲击大的特点，把经营模式定位为薄利多销型</td>\n",
       "      <td>削价/低价出售(undercuts):-0.3,削价/低价出售(undercut):-0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...</td>\n",
       "      <td>{\"情绪得分\": \"-0.5\"}</td>\n",
       "      <td>好的，我现在需要分析给定的新闻文本的情绪，并根据提供的评分标准给出五档制中的得分。首先，我得...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>**角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...</td>\n",
       "      <td>{\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{\"情绪分析\":\"①语义信息为铝型材市场受塑钢冲击大、企业生产规模小等不利因素下被迫采取薄利...</td>\n",
       "      <td>好的，我现在需要分析用户提供的新闻文本，并根据给定的任务流程和评分逻辑来确定情绪得分。首先，...</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                              Title NewsSource yearmonth  \\\n",
       "0            11                   评论：一份有情怀有思路的政府报告     第一财经日报    15-Mar   \n",
       "1             6  基金年度“成绩单”角逐进入白热化 绩优产品多布局传媒消费通讯等领域      经济参考报    23-Nov   \n",
       "2             0            元旦谁抢中国游客多？日本韩国，一个笑了一个哭了     每日经济新闻    18-Jan   \n",
       "3             8      又一大省放开落户！除两城外，山东其余14城或全部“零门槛”   21世纪经济报道    20-Jan   \n",
       "4            12      专访顾云昌：北京共有产权房是1998年房改后的又一重大创新     中国经济周刊    17-Aug   \n",
       "..          ...                                ...        ...       ...   \n",
       "493         492             赢创：创新环保及城镇化将成为今后发展三大主题     第一财经日报    13-Apr   \n",
       "494         482                “七连涨”遇大跳水 融资盘企稳再受考验     第一财经日报    15-Aug   \n",
       "495         498              ​积极推进新型城镇化，各类要素如何双向流动     中国经济周刊    24-Apr   \n",
       "496         496                            村里超市开业了       经济日报     7-Jan   \n",
       "497         499    在中央政治局常委会会议研究应对新型冠状病毒肺炎疫情工作时的讲话       经济日报    20-Feb   \n",
       "\n",
       "                          stratum   topic  similarity  \\\n",
       "0      2015-03-06 00:00:00_第一财经日报  “宏观经济”    0.543562   \n",
       "1       2023-11-28 00:00:00_经济参考报    “金融”    0.501491   \n",
       "2      2018-01-02 00:00:00_每日经济新闻    “消费”    0.611313   \n",
       "3    2020-01-13 00:00:00_21世纪经济报道   “房地产”    0.534588   \n",
       "4      2017-08-11 17:35:00_中国经济周刊   “房地产”    0.549985   \n",
       "..                            ...     ...         ...   \n",
       "493    2013-04-03 00:00:00_第一财经日报  “生产投资”    0.482415   \n",
       "494    2015-08-19 00:00:00_第一财经日报    “金融”    0.546733   \n",
       "495    2024-04-30 15:37:00_中国经济周刊   “房地产”    0.634848   \n",
       "496      2007-01-14 00:00:00_经济日报    “消费”    0.547639   \n",
       "497      2020-02-16 00:00:00_经济日报  “生产投资”    0.516709   \n",
       "\n",
       "                                                prompt  \\\n",
       "0    新闻文本是##“总理的报告，在解题思路方面亮点纷呈”##，包含的情绪词典是##：解决/解答(...   \n",
       "1    新闻文本是##“截至目前，共989只基金成立以来的回报率在100%以上”##，包含的情绪词典...   \n",
       "2    新闻文本是##“携程跟团游、自由行数据，香港、曼谷、东京、新加坡、大阪、长滩岛、台北、芽庄、...   \n",
       "3    新闻文本是##“生产要素包括人口的流动还会更加自由，所以城镇化绝对不是一个就地城镇化的概念”...   \n",
       "4    新闻文本是##“而保障房可分为两种：公租房，以及新推出的共有产权房”##，包含的情绪词典是#...   \n",
       "..                                                 ...   \n",
       "493  新闻文本是##“环保标准全球统一\\n　　研发新产品是一家创新型特种化学品企业的核心要素之一”...   \n",
       "494  新闻文本是##“当8月18日大盘调整再度来袭时，资金面信心也将再次接受严峻考验”##，包含的...   \n",
       "495  新闻文本是##“规划时，一些地方官员没有考虑到当地的具体情况，易导致出现不良资产，尤其是个别...   \n",
       "496  新闻文本是##“现在农村有了超市，好多假货彻底失去了市场”##，包含的情绪词典是##：无销路...   \n",
       "497  新闻文本是##“以前，针对原材料价格低、企业生产规模小、铝型材巿场受塑钢冲击大的特点，把经营...   \n",
       "\n",
       "                                              cut_news  \\\n",
       "0                                    总理的报告，在解题思路方面亮点纷呈   \n",
       "1                          截至目前，共989只基金成立以来的回报率在100%以上   \n",
       "2    携程跟团游、自由行数据，香港、曼谷、东京、新加坡、大阪、长滩岛、台北、芽庄、迪拜、伊斯坦布尔...   \n",
       "3                生产要素包括人口的流动还会更加自由，所以城镇化绝对不是一个就地城镇化的概念   \n",
       "4                            而保障房可分为两种：公租房，以及新推出的共有产权房   \n",
       "..                                                 ...   \n",
       "493              环保标准全球统一\\n　　研发新产品是一家创新型特种化学品企业的核心要素之一   \n",
       "494                    当8月18日大盘调整再度来袭时，资金面信心也将再次接受严峻考验   \n",
       "495  规划时，一些地方官员没有考虑到当地的具体情况，易导致出现不良资产，尤其是个别人口流出地区，还...   \n",
       "496                               现在农村有了超市，好多假货彻底失去了市场   \n",
       "497   以前，针对原材料价格低、企业生产规模小、铝型材巿场受塑钢冲击大的特点，把经营模式定位为薄利多销型   \n",
       "\n",
       "                                        sentiment_dict  ...  \\\n",
       "0                 解决/解答(solve):0.3,问题/难题(problem):-0.4  ...   \n",
       "1                  百分比(percentage):0.0,基金/资金(fund):0.1  ...   \n",
       "2           海外的/国外的(overseas):0.0,外国的/国外的(foreign):0.0  ...   \n",
       "3       自由的/免费的(free):0.2,非流动性/缺乏流动性(illiquidity):-0.5  ...   \n",
       "4                  房子/住宅(house):0.0,房地产/财产(estate):0.0  ...   \n",
       "..                                                 ...  ...   \n",
       "493  创新/革新(innovation):0.7,创新性/革新性(innovativeness):0.8  ...   \n",
       "494    资金不足的(underfunded):-0.3,衰退/不景气(recession):-0.95  ...   \n",
       "495  未充分利用(underutilization):-0.3,管理不善/错误管理(mismana...  ...   \n",
       "496    无销路的/滞销的(unmarketable):-0.5,滞销(unsellable):-0.4  ...   \n",
       "497     削价/低价出售(undercuts):-0.3,削价/低价出售(undercut):-0.3  ...   \n",
       "\n",
       "                                            qwen_input          Response  \\\n",
       "0    **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...   {\"情绪得分\": \"0.5\"}   \n",
       "1    **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...   {\"情绪得分\": \"0.5\"}   \n",
       "2    **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...     {\"情绪得分\": \"0\"}   \n",
       "3    **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...     {\"情绪得分\": \"0\"}   \n",
       "4    **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...     {\"情绪得分\": \"0\"}   \n",
       "..                                                 ...               ...   \n",
       "493  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...   {\"情绪得分\": \"0.5\"}   \n",
       "494  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...  {\"情绪得分\": \"-0.5\"}   \n",
       "495  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...  {\"情绪得分\": \"-0.5\"}   \n",
       "496  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...     {\"情绪得分\": \"1\"}   \n",
       "497  **角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的...  {\"情绪得分\": \"-0.5\"}   \n",
       "\n",
       "                                           Complex_CoT label  \\\n",
       "0    好的，我现在需要分析这段新闻文本的情绪得分。首先，新闻内容是关于总理的报告在解题思路方面有亮...   0.5   \n",
       "1    好的，我来仔细分析一下这个新闻文本的情绪。首先，新闻内容是：“截至目前，共989只基金成立以...   0.5   \n",
       "2    好的，我现在需要分析这条新闻的情绪得分。首先，看看新闻内容。主要讲的是携程的跟团游和自由行数...   0.0   \n",
       "3    好的，我现在需要分析给定的新闻文本的情绪得分。首先，看一下新闻内容：“生产要素包括人口的流动...   0.0   \n",
       "4    好，我需要分析这段关于中国保障房的新闻文本的情绪。首先，看整体内容。新闻提到保障房分为公租房...   0.0   \n",
       "..                                                 ...   ...   \n",
       "493  好的，我现在需要处理这个用户的查询。首先，用户给了一个任务，要求作为中英双语市场情绪分析专家...   0.0   \n",
       "494  好的，我需要分析这条新闻的情绪，并给出五档评分。首先看新闻内容：“当8月18日大盘调整再度来...  -0.5   \n",
       "495  好的，我现在需要分析这篇新闻文本的情绪得分。首先，新闻内容是关于地方官员在规划时没有考虑当地...  -0.5   \n",
       "496  好的，我需要分析这条新闻的情绪。新闻内容是：“现在农村有了超市，好多假货彻底失去了市场。”首...   1.0   \n",
       "497  好的，我现在需要分析给定的新闻文本的情绪，并根据提供的评分标准给出五档制中的得分。首先，我得...   0.0   \n",
       "\n",
       "                                              Question  \\\n",
       "0    **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "1    **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "2    **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "3    **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "4    **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "..                                                 ...   \n",
       "493  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "494  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "495  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "496  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "497  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...   \n",
       "\n",
       "                                           qwen_output qwen_score  \\\n",
       "0    {\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪得分。首先，用...        1.0   \n",
       "1    {\"情绪分析\":\"好的，我需要分析这条新闻的情绪，并给出五档评分。首先看新闻内容：“截至目前...        0.5   \n",
       "2    {\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...        1.0   \n",
       "3    {\"情绪分析\":\"好的，我现在需要分析给定的新闻文本的情绪，并根据提供的情绪词典给出五档制评...        0.5   \n",
       "4    {\"情绪分析\":\"好的，我现在需要处理用户提供的这个市场情绪分析任务。首先，我得仔细阅读用户...        0.0   \n",
       "..                                                 ...        ...   \n",
       "493  {\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...        0.5   \n",
       "494  {\"情绪分析\":\"好的，我现在需要分析给定的新闻文本，并根据提供的情绪词典和评分规则给出情绪...       -0.5   \n",
       "495  {\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...       -1.0   \n",
       "496  {\"情绪分析\":\"好的，我需要分析这段新闻文本的情绪，并给出五档制评分。首先看新闻内容：“现...       -0.5   \n",
       "497  {\"情绪分析\":\"好的，我现在需要处理这个用户的查询，分析给定的新闻文本的情绪，并给出五档制...        0.0   \n",
       "\n",
       "                                               content  \\\n",
       "0    {\"情绪分析\":\"①语义信息强调总理报告的解题思路存在'亮点纷呈'，突出其策略创新性与解决方...   \n",
       "1    {\"情绪分析\":\"①语义信息为近千只基金成立以来的回报率超过100%，展示出较强的长期收益能...   \n",
       "2    {\"情绪分析\":\"①语义信息为列举中国游客境外热门旅游目的地及购物季对城市人气的提升效应，属...   \n",
       "3    {\"情绪分析\":\"①语义信息强调生产要素与人口流动将更加自由，并明确否定就地城镇化的局限性，...   \n",
       "4    {\"情绪分析\":\"①语义信息为对保障房类型的客观分类说明，未体现政策效果或市场影响的价值判断...   \n",
       "..                                                 ...   \n",
       "493  {\"情绪分析\":\"①语义信息为强调环保标准全球统一及创新型企业的研发重要性，前者隐含标准化带...   \n",
       "494  {\"情绪分析\":\"①语义信息为大盘调整再度来袭导致资金面信心面临严峻考验，暗示市场波动加剧及...   \n",
       "495  {\"情绪分析\":\"①语义信息为地方官员规划忽视当地实情导致不良资产及人口流出地区资源过度投入...   \n",
       "496  {\"情绪分析\":\"①语义信息为农村超市普及导致假货市场消失，反映消费环境优化与正品流通保障的...   \n",
       "497  {\"情绪分析\":\"①语义信息为铝型材市场受塑钢冲击大、企业生产规模小等不利因素下被迫采取薄利...   \n",
       "\n",
       "                                   reasoning_content\\r ds_api_score  \n",
       "0    好的，我现在需要分析这个新闻文本的情绪，并根据给定的情绪词典给出评分。首先，新闻文本是：“总...          0.5  \n",
       "1    好的，我需要分析这条新闻文本的情绪，并给出对应的五档制评分。首先，看看新闻内容：“截至目前，...          0.5  \n",
       "2    好的，我现在需要分析这个新闻文本的情绪，并根据给定的任务流程和评分逻辑给出结果。首先，我要仔...          0.0  \n",
       "3    好的，我现在需要分析这条新闻文本的情绪，并给出对应的五档制评分。首先，我得仔细阅读新闻内容，...          0.5  \n",
       "4    好的，我现在要处理这条新闻文本的情绪分析任务。首先，我需要仔细阅读新闻内容：“而保障房可分为...          0.0  \n",
       "..                                                 ...          ...  \n",
       "493  好，我现在要分析这个新闻文本的情绪得分。首先，新闻文本是：“环保标准全球统一 研发新产品是一...          0.5  \n",
       "494  好的，我现在需要分析用户提供的新闻文本，并按照给定的任务流程和评分逻辑来给出情绪分析和对应的...         -0.5  \n",
       "495  好的，我来仔细分析一下这个新闻文本。首先，文本提到地方官员在规划时没有考虑当地具体情况，导致...         -0.5  \n",
       "496  好的，我来仔细分析一下这个新闻文本。首先，新闻内容是“现在农村有了超市，好多假货彻底失去了市...          0.5  \n",
       "497  好的，我现在需要分析用户提供的新闻文本，并根据给定的任务流程和评分逻辑来确定情绪得分。首先，...         -0.5  \n",
       "\n",
       "[498 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\"情绪得分\": \"(-?\\d+(?:\\.\\d+)?)\"'\n",
    "def extract_score(text):\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return (match.group(1))\n",
    "    return None\n",
    "\n",
    "# 应用函数到 Response 列\n",
    "test['ds_score'] = test['Response'].apply(extract_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有 ds_api_sds_scorecore 的值都合法。\n"
     ]
    }
   ],
   "source": [
    "valid_scores = {-1.0, -0.5, 0.0, 0.5, 1.0}\n",
    "\n",
    "# 转换为 float，确保是数值型\n",
    "test['ds_score'] = pd.to_numeric(test['ds_score'], errors='coerce')\n",
    "\n",
    "# 检查是否存在非法得分\n",
    "invalid_scores = test[~test['ds_score'].isin(valid_scores)]\n",
    "\n",
    "# 输出结果\n",
    "if invalid_scores.empty:\n",
    "    print(\"✅ 所有 ds_api_sds_scorecore 的值都合法。\")\n",
    "else:\n",
    "    print(f\"❌ 存在非法得分，共 {len(invalid_scores)} 条记录：\")\n",
    "    print(invalid_scores[['qwen_output', 'ds_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53335/1277363253.py\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatching_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcount_matching_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'qwen_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_treatment_for_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_53335/1277363253.py\u001b[0m in \u001b[0;36mcount_matching_rows\u001b[0;34m(test, ds_score_col, qwen_score_col, special_treatment_for_zero)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 不考虑对 0 的特殊处理，仅判断差值绝对值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mconditions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_score_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqwen_score_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 筛选符合条件的行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__sub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_method_SERIES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# Don't do this for comparisons, as that will handle complex numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m#  incorrectly, see GH#32047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_masked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/news_emo/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_matching_rows(test, ds_score_col, qwen_score_col, special_treatment_for_zero=True):\n",
    "    if special_treatment_for_zero:\n",
    "        # 定义条件，考虑对 0 的特殊处理\n",
    "        conditions = (\n",
    "            ((test[ds_score_col] == 0) & (test[qwen_score_col] == 0)) | \n",
    "            ((test[ds_score_col] != 0) & ((test[ds_score_col] - test[qwen_score_col]).abs() <= 0.5))\n",
    "        )\n",
    "    else:\n",
    "        # 不考虑对 0 的特殊处理，仅判断差值绝对值\n",
    "        conditions = (test[ds_score_col] - test[qwen_score_col]).abs() <= 0.5\n",
    "\n",
    "    # 筛选符合条件的行\n",
    "    matching_rows = test[conditions]\n",
    "\n",
    "    # 计算匹配行数\n",
    "    matching_count = len(matching_rows)\n",
    "    print(matching_count/498)\n",
    "\n",
    "    return matching_count\n",
    "\n",
    "count_matching_rows(test,'label','qwen_score', special_treatment_for_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_excel('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/wxy_change_label.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"**角色定义**\n",
    "你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的新闻文本情绪进行准确的评分：\n",
    "\n",
    "**任务流程**\n",
    "1. 分析新闻整体内容情绪\n",
    "2. 匹配情绪词典关键词，对情绪进行深入理解与分析\n",
    "3. 输出情绪分析跟对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0，分别代表非常消极、比较消极、中性、比较积极、非常积极）\n",
    "\n",
    "**评分逻辑**\n",
    "1. 语义匹配：忽略偏离的情绪词词汇，保留有效情绪词\n",
    "2. 评分调整：整体语义优先，情绪词辅助修正\n",
    "\n",
    "**示例说明**\n",
    "- 示例1：\n",
    "新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\n",
    "情绪词典：profitability:0.6,profit:0.8\n",
    "情绪分析:①语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破；②关键情绪词调整：“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1.0分（非常积极）的评分结果。\n",
    "情绪得分:1.0\n",
    "\n",
    "- 示例2：\n",
    "新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\n",
    "情绪词典：mispricing:-0.4,advantage:0.7\n",
    "情绪分析:①语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号；②关键情绪词调整：未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。由于缺乏词典强匹配项限制进一步上调空间，整体乐观基调符合“比较积极”，情绪得分0.5。\n",
    "情绪得分:0.5\n",
    "\n",
    "- 示例3：\n",
    "新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\n",
    "情绪词典：free:0.2,immateriality:-0.2\n",
    "情绪分析:①语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响；②关键情绪词调整：“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0.0分。\n",
    "情绪得分:0.0\n",
    "\n",
    "- 示例4：\n",
    "新闻文本：我们投入的前期费用谁来承担\n",
    "情绪词典：invest:0.3\n",
    "情绪分析:①语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪；②关键情绪词调整：“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位“比较消极”，即情绪得分-0.5分。\n",
    "情绪得分:-0.5\n",
    "\n",
    "- 示例5：\n",
    "新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\n",
    "情绪词典：crash:-0.9,meltdown:-0.8\n",
    "情绪分析:①语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词调整：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，因此是非常消极，情绪得分-1.0分。\n",
    "情绪得分:-1.0\n",
    "\n",
    "\n",
    "**其他说明**\n",
    "- 情绪词典的分值仅作语义方向参考\n",
    "- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分数\n",
    "- 输出格式：{\"情绪分析\":\"...\",\"情绪得分\":\"...\"}\n",
    "\n",
    "现在，请你开始分析并按照要求输出结果：\n",
    "新闻文本：{cut_news}\n",
    "情绪词典：{sentiment_dict_v2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data['qwen_input'] = None\n",
    "\n",
    "# 循环遍历每一行\n",
    "for index, row in sampled_data.iterrows():\n",
    "    cut_news = row['cut_news']\n",
    "    sentiment_dict_v2 = row['sentiment_dict_v2']\n",
    "    filled_template = prompt_template.replace('{cut_news}', cut_news).replace('{sentiment_dict_v2}', sentiment_dict_v2)\n",
    "    sampled_data.at[index, 'qwen_input'] = filled_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "test1 = pd.read_csv('/root/LLM_news_emo_analyze/DATA/qwen_sft_train_0330data/qwen_test_data.csv',encoding = 'utf_8_sig',lineterminator='\\n')\n",
    "test2 = pd.read_csv('/root/LLM_news_emo_analyze/DATA/both_model_train_data_0430/both_test_data_processed_2_processed_processed.csv',encoding = 'utf_8_sig',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DeclareDate                                      Title  \\\n",
      "2683  2018-04-02 00:00:00                                第三批自贸试验区周年考   \n",
      "2689  2016-10-31 15:42:00                         5575万人脱贫的“硬骨头”如何啃?   \n",
      "2577  2019-07-05 23:34:00                              “跟投大法”大规模受阻背后   \n",
      "1048  2019-03-02 00:00:00           外卖用户规模已高达3.6亿新零售跨界致外卖市场竞争格局进入新阶段   \n",
      "2568  2019-03-12 00:00:00                                 补齐农村基础设施短板   \n",
      "...                   ...                                        ...   \n",
      "2019  2010-01-15 00:00:00                         褐皮书“示好” 美联储或布局退出政策   \n",
      "941   2011-04-07 00:00:00                            鼎立股份稀土概念“画饼充饥”？   \n",
      "2533  2023-08-02 00:00:00                           天健集团：接受财通证券等机构调研   \n",
      "1984  2015-04-03 00:00:00                                      ■行业资讯   \n",
      "1459  2017-01-09 00:00:00  金融危机以来全球690次降息后 2017转变开始：货币刺激退居二线财政政策走上前台   \n",
      "\n",
      "                                            NewsContent NewsSource yearmonth  \\\n",
      "2683  最是一年春好处，何况是周年到来之日。2017年4月1日，中国第三批自贸试验区挂牌，吹响了新一...     每日经济新闻   2018-04   \n",
      "2689  （本文刊发于《中国经济周刊》2016年第42期）\\n《中国经济周刊》 记者 王红茹 | 北京...     中国经济周刊   2016-10   \n",
      "2577  陈博“现在万科员工跟投都跟不动了，压力挺大，全国前50强房企，接近30家设置了跟投机制，都出...      经济观察报   2019-07   \n",
      "1048  随着消费升级大潮和新零售的影响，本地生活行业正在持续稳定增长，其中，网络外卖在经历2018年...     每日经济新闻   2019-03   \n",
      "2568   政府工作报告提出，扎实推进乡村建设。因地制宜开展农村人居环境整治，推进“厕所革命”、垃圾污...       经济日报   2019-03   \n",
      "...                                                 ...        ...       ...   \n",
      "2019  　　美国时间周三下午，美联储公布了最新一期的褐皮书报告，报告显示美国经济在2009年底持续小...     第一财经日报   2010-01   \n",
      "941   又是稀土。\\n\\n\\n\\n自去年12月10日6.42元以来不明原因的逆势大涨，至2月18日发...   21世纪经济报道   2011-04   \n",
      "2533  天健集团（SZ 000090，收盘价：6.2元）发布公告称，2023年8月1日，天健集团接受...     每日经济新闻   2023-08   \n",
      "1984   加强“候鸟型”家庭电费回收\\n “请帮我预存500元电费，我浇完这块麦田就要外出务工了，这...      经济参考报   2015-04   \n",
      "1459  编者按\\n风波不断的2016年终于过去，崭新的2017年已经到来。新年伊始，随着全球再通胀预...   21世纪经济报道   2017-01   \n",
      "\n",
      "                           stratum   topic  similarity  \\\n",
      "2683    2018-04-02 00:00:00_每日经济新闻  “生产投资”    0.499663   \n",
      "2689    2016-10-31 15:42:00_中国经济周刊   “房地产”    0.510408   \n",
      "2577     2019-07-05 23:34:00_经济观察报   “房地产”    0.501065   \n",
      "1048    2019-03-02 00:00:00_每日经济新闻    “消费”    0.504904   \n",
      "2568      2019-03-12 00:00:00_经济日报    “金融”    0.484307   \n",
      "...                            ...     ...         ...   \n",
      "2019    2010-01-15 00:00:00_第一财经日报    “金融”    0.542196   \n",
      "941   2011-04-07 00:00:00_21世纪经济报道    “金融”    0.593858   \n",
      "2533    2023-08-02 00:00:00_每日经济新闻    “消费”    0.518268   \n",
      "1984     2015-04-03 00:00:00_经济参考报  “生产投资”    0.500398   \n",
      "1459  2017-01-09 00:00:00_21世纪经济报道  “通货膨胀”    0.514042   \n",
      "\n",
      "                                                 prompt  \\\n",
      "2683  新闻文本是##“浙辽：沿海开放新阵地 　　浙江自贸试验区将三个片区全部放在了一个城市——舟山...   \n",
      "2689  新闻文本是##““版权大战”背后的“圈地大战”\\n2015年7月，国家版权局下发《关于责令网...   \n",
      "2577  新闻文本是##“收益降低，甚至出现亏损项目——是近年来房企员工“跟不动”的主要原因”##，包...   \n",
      "1048  新闻文本是##“同时美团外卖、饿了么和饿了么星选在网络外卖服务用户中使用率最高”##，包含的...   \n",
      "2568  新闻文本是##“上海率先做出响应”##，包含的情绪词典是##：积极主动的/先发制人的(pro...   \n",
      "...                                                 ...   \n",
      "2019  新闻文本是##“再以个股嘉汉林业为例，Data Explorers的数据显示，去年底做空嘉汉...   \n",
      "941   新闻文本是##“这是人民币跨境贸易结算试点以来我国发生的首笔跨境人民币账户融资业务，意味着人...   \n",
      "2533  新闻文本是##“由于公路多年失修，路况较差，交通事故也时常发生，中国平安建议，去巴西看球的球...   \n",
      "1984  新闻文本是##“35千伏义渡变电站检修期间，分为15个检修区段，共有生产、安全相关部室及14...   \n",
      "1459  新闻文本是##“美国的工资增速正在加快，中国的PPI显示通胀回归，欧元区的增长也上了轨道”#...   \n",
      "\n",
      "                                               cut_news  \\\n",
      "2683             浙辽：沿海开放新阵地 　　浙江自贸试验区将三个片区全部放在了一个城市——舟山   \n",
      "2689  “版权大战”背后的“圈地大战”\\n2015年7月，国家版权局下发《关于责令网络音乐服务商停止...   \n",
      "2577                  收益降低，甚至出现亏损项目——是近年来房企员工“跟不动”的主要原因   \n",
      "1048                    同时美团外卖、饿了么和饿了么星选在网络外卖服务用户中使用率最高   \n",
      "2568                                           上海率先做出响应   \n",
      "...                                                 ...   \n",
      "2019  再以个股嘉汉林业为例，Data Explorers的数据显示，去年底做空嘉汉林业的空头仓位是...   \n",
      "941   这是人民币跨境贸易结算试点以来我国发生的首笔跨境人民币账户融资业务，意味着人民币贸易结算试点...   \n",
      "2533  由于公路多年失修，路况较差，交通事故也时常发生，中国平安建议，去巴西看球的球迷如需乘坐长途公...   \n",
      "1984  35千伏义渡变电站检修期间，分为15个检修区段，共有生产、安全相关部室及14个供电所、152...   \n",
      "1459               美国的工资增速正在加快，中国的PPI显示通胀回归，欧元区的增长也上了轨道   \n",
      "\n",
      "                                         sentiment_dict  \\\n",
      "2683             地区的/区域的(regional):0.0,贸易/交易(trade):0.0   \n",
      "2689            侵权地(tortiously):-0.4,侵权的(tortious):-0.5   \n",
      "2577  表现不佳/业绩低于预期(underperforming):-0.4,表现不佳/业绩低于预期(...   \n",
      "1048               消费者/用户(consumer):0.0,食物/食品(food):0.0   \n",
      "2568  积极主动的/先发制人的(proactive):0.6,积极主动地/先发制人地(proacti...   \n",
      "...                                                 ...   \n",
      "2019         清仓/抛售(closeouts):-0.2,清仓/抛售(closeout):-0.2   \n",
      "941          突破/重大进展(breakthrough):0.5,贸易/交易(trade):0.0   \n",
      "2533        事故/意外(accident):-0.3,有缺陷的/有瑕疵的(flawed):-0.3   \n",
      "1984     电的/电气的(electrical):0.0,停工期/故障时间(downtime):-0.3   \n",
      "1459             欧元区(eurozone):0.0,通货膨胀(inflation):-0.5   \n",
      "\n",
      "                            sentiment_dict_v2  \\\n",
      "2683                  regional:0.0, trade:0.0   \n",
      "2689           tortiously:-0.4, tortious:-0.5   \n",
      "2577  underperforming:-0.4, underperform:-0.4   \n",
      "1048                   consumer:0.0, food:0.0   \n",
      "2568           proactive:0.6, proactively:0.5   \n",
      "...                                       ...   \n",
      "2019            closeouts:-0.2, closeout:-0.2   \n",
      "941               breakthrough:0.5, trade:0.0   \n",
      "2533               accident:-0.3, flawed:-0.3   \n",
      "1984            electrical:0.0, downtime:-0.3   \n",
      "1459             eurozone:0.0, inflation:-0.5   \n",
      "\n",
      "                                             qwen_input  \n",
      "2683  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "2689  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "2577  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "1048  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "2568  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "...                                                 ...  \n",
      "2019  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "941   **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "2533  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "1984  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "1459  **角色定义**\\n擅长中英双语的市场情绪分析专家，能够对中国市场的新闻文本情绪进行准确的评...  \n",
      "\n",
      "[502 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# 找出在 test1 但不在 test2 中的数据\n",
    "unique_data = test1[~test1['cut_news'].isin(test2['cut_news'])]\n",
    "\n",
    "# 随机抽取 502 条数据\n",
    "if len(unique_data) >= 502:\n",
    "    sampled_data = unique_data.sample(n=502)\n",
    "else:\n",
    "    print(\"满足条件的数据不足 502 条，只能抽取 {} 条数据。\".format(len(unique_data)))\n",
    "    sampled_data = unique_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Title', 'NewsSource', 'yearmonth', 'stratum', 'topic',\n",
       "       'similarity', 'prompt', 'cut_news', 'sentiment_dict',\n",
       "       'sentiment_dict_v2', 'qwen_input', 'Response', 'Complex_CoT', 'label',\n",
       "       'Question', 'qwen_output', 'qwen_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col = ['Title', 'NewsSource', 'yearmonth', 'stratum', 'topic',\n",
    "       'similarity', 'prompt', 'cut_news', 'sentiment_dict',\n",
    "       'sentiment_dict_v2','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DeclareDate', 'Title', 'NewsContent', 'NewsSource', 'yearmonth',\n",
       "       'stratum', 'topic', 'similarity', 'prompt', 'cut_news',\n",
       "       'sentiment_dict', 'sentiment_dict_v2', 'qwen_input'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = test1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**角色定义**\\n你是一位擅长中英双语的中国市场情绪分析专家，能够基于任务流程对中国市场的新闻文本情绪进行准确的评分：\\n\\n**任务流程**\\n1. 分析新闻整体内容情绪\\n2. 匹配情绪词典关键词，对情绪进行深入理解与分析\\n3. 输出情绪分析跟对应的五档制评分结果（-1.0/-0.5/0.0/0.5/1.0，分别代表非常消极、比较消极、中性、比较积极、非常积极）\\n\\n**评分逻辑**\\n1. 语义匹配：忽略偏离的情绪词词汇，保留有效情绪词\\n2. 评分调整：整体语义优先，情绪词辅助修正\\n\\n**示例说明**\\n- 示例1：\\n新闻文本：实现净利润同比增长137.98%，单季度的盈利规模超过中信证券成为业内第一\\n情绪词典：profitability:0.6,profit:0.8\\n情绪分析:①语义信息为净利润同比大幅增长137.98%及单季度盈利规模跃居行业第一，均体现超预期的盈利能力突破；②关键情绪词调整：“净利润”（匹配profit）和“盈利”（匹配profitability）共同强化积极方向。两重强信号叠加符合最高档1.0分（非常积极）的评分结果。\\n情绪得分:1.0\\n\\n- 示例2：\\n新闻文本：中国的A股定位反而是比较便宜的，外资从全球定价认为我们非常有吸引力\\n情绪词典：mispricing:-0.4,advantage:0.7\\n情绪分析:①语义信息为A股估值被强调为“便宜”及外资认可其全球定价吸引力，隐含市场价值被低估的积极信号；②关键情绪词调整：未直接匹配词典中的“mispricing”或“advantage”，但“便宜”隐含定价偏离逻辑（映射mispricing方向），“有吸引力”间接呼应优势（advantage方向）。由于缺乏词典强匹配项限制进一步上调空间，整体乐观基调符合“比较积极”，情绪得分0.5。\\n情绪得分:0.5\\n\\n- 示例3：\\n新闻文本：美国信奉自由市场经济理念，主张靠无形的手调整经济活动\\n情绪词典：free:0.2,immateriality:-0.2\\n情绪分析:①语义信息为对美国经济理念的中性陈述，既未直接关联中国市场优劣，也未体现政策对华影响；②关键情绪词调整：“自由”（匹配free:+0.2）与“无形”（匹配immateriality:-0.2）存在方向冲突，但文本未实际使用“immateriality”原词（仅隐含“无形的手”概念），语义匹配强度不足。陈述性内容缺乏明确情绪导向，符合中性基准0.0分。\\n情绪得分:0.0\\n\\n- 示例4：\\n新闻文本：我们投入的前期费用谁来承担\\n情绪词典：invest:0.3\\n情绪分析:①语义信息为对前期费用承担主体的质疑，隐含投入成本未被消化的潜在风险，传递财务负担不确定性的负面情绪；②关键情绪词调整：“投入”（匹配invest:+0.3）存在方向性冲突，因文本中“投入”实际指向成本分摊压力而非正向投资预期，情绪词得分被整体语义逆向修正。中性词主导+隐含担忧的复合信号符合低度负面评分档位“比较消极”，即情绪得分-0.5分。\\n情绪得分:-0.5\\n\\n- 示例5：\\n新闻文本：饱受美国次贷危机冲击的华尔街再次风云突变\\n情绪词典：crash:-0.9,meltdown:-0.8\\n情绪分析:①语义信息为华尔街受次贷危机冲击引发的市场动荡，此类全球金融中心的不稳定通常导致跨国资本避险情绪上升，对中国市场构成外溢风险；②关键情绪词调整：未直接匹配“crash”或“meltdown”，但“次贷危机冲击”与“风云突变”共同映射系统性风险（贴近meltdown的-0.8方向），叠加事件严重性突破常规调整范畴。极端负面事件的整体语义强度主导评分，因此是非常消极，情绪得分-1.0分。\\n情绪得分:-1.0\\n\\n\\n**其他说明**\\n- 情绪词典的分值仅作语义方向参考\\n- 情绪得分必须是五档制选择，不得出现-1.0/-0.5/0.0/0.5/1.0之外的分数\\n- 输出格式：{\"情绪分析\":\"...\",\"情绪得分\":\"...\"}\\n\\n现在，请你开始分析并按照要求输出结果：\\n新闻文本：收益降低，甚至出现亏损项目——是近年来房企员工“跟不动”的主要原因\\n情绪词典：underperforming:-0.4, underperform:-0.4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.iloc[2,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
